timestamp,message
1740517632370,"INFO	2025-02-25T21:07:12,370	2545	com.amazon.ws.emr.hadoop.fs.util.PlatformInfo	[main]	63	Unable to read clusterId from /var/lib/instance-controller/extraInstanceData.json, trying EMR job-flow data file: /var/lib/info/job-flow.json
"
1740517632371,"INFO	2025-02-25T21:07:12,370	2545	com.amazon.ws.emr.hadoop.fs.util.PlatformInfo	[main]	71	Unable to read clusterId from /var/lib/info/job-flow.json, out of places to look
"
1740517632833,"WARN	2025-02-25T21:07:12,832	3007	com.amazonaws.util.VersionInfoUtils	[main]	85	The AWS SDK for Java 1.x entered maintenance mode starting July 31, 2024 and will reach end of support on December 31, 2025. For more information, see https://aws.amazon.com/blogs/developer/the-aws-sdk-for-java-1-x-is-in-maintenance-mode-effective-july-31-2024/
You can print where on the file system the AWS SDK for Java 1.x core runtime is located by setting the AWS_JAVA_V1_PRINT_LOCATION environment variable or aws.java.v1.printLocation system property to 'true'.
This message can be disabled by setting the AWS_JAVA_V1_DISABLE_DEPRECATION_ANNOUNCEMENT environment variable or aws.java.v1.disableDeprecationAnnouncement system property to 'true'.
The AWS SDK for Java 1.x is being used here:
at java.base/java.lang.Thread.getStackTrace(Thread.java:1619)
at com.amazonaws.util.VersionInfoUtils.printDeprecationAnnouncement(VersionInfoUtils.java:81)
at com.amazonaws.util.VersionInfoUtils.<clinit>(VersionInfoUtils.java:59)
at com.amazonaws."
1740517632833,"internal.EC2ResourceFetcher.<clinit>(EC2ResourceFetcher.java:44)
at com.amazonaws.auth.InstanceMetadataServiceCredentialsFetcher.<init>(InstanceMetadataServiceCredentialsFetcher.java:38)
at com.amazonaws.auth.InstanceProfileCredentialsProvider.<init>(InstanceProfileCredentialsProvider.java:111)
at com.amazonaws.auth.InstanceProfileCredentialsProvider.<init>(InstanceProfileCredentialsProvider.java:91)
at com.amazonaws.auth.InstanceProfileCredentialsProvider.<init>(InstanceProfileCredentialsProvider.java:75)
at com.amazonaws.auth.InstanceProfileCredentialsProvider.<clinit>(InstanceProfileCredentialsProvider.java:58)
at com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper.initializeProvider(EC2ContainerCredentialsProviderWrapper.java:66)
at com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper.<init>(EC2ContainerCredentialsProviderWrapper.java:55)
at com.amazonaws.auth.DefaultAWSCredentialsProviderChain.<init>(DefaultAWSCredentialsProviderChain.java:60)
at com.amazonaws.auth.DefaultAWSCredentialsProvide"
1740517632834,"rChain.<clinit>(DefaultAWSCredentialsProviderChain.java:54)
at java.base/java.lang.Class.forName0(Native Method)
at java.base/java.lang.Class.forName(Class.java:375)
at com.amazon.ws.emr.hadoop.fs.guice.DefaultAWSCredentialsProviderFactory.getCustomAwsCredentialsProvider(DefaultAWSCredentialsProviderFactory.java:61)
at com.amazon.ws.emr.hadoop.fs.guice.DefaultAWSCredentialsProviderFactory.getAwsCredentialsProviderChain(DefaultAWSCredentialsProviderFactory.java:32)
at com.amazon.ws.emr.hadoop.fs.guice.DefaultAWSCredentialsProviderFactory.getAwsCredentialsProvider(DefaultAWSCredentialsProviderFactory.java:26)
at com.amazon.ws.emr.hadoop.fs.guice.EmrFSProdModule.getAwsCredentialsProvider(EmrFSProdModule.java:81)
at com.amazon.ws.emr.hadoop.fs.guice.EmrFSProdModule.createAmazonS3LiteClient(EmrFSProdModule.java:93)
at com.amazon.ws.emr.hadoop.fs.guice.EmrFSProdModule.createAmazonS3Lite(EmrFSProdModule.java:266)
at com.amazon.ws.emr.hadoop.fs.guice.EmrFSBaseModule.provideAmazonS3Lite(EmrFSBaseModule.java:112)
at co"
1740517632834,"m.amazon.ws.emr.hadoop.fs.guice.EmrFSBaseModule$$FastClassByGuice$$1921810.GUICE$TRAMPOLINE(<generated>)
at com.amazon.ws.emr.hadoop.fs.guice.EmrFSBaseModule$$FastClassByGuice$$1921810.apply(<generated>)
at com.amazon.ws.emr.hadoop.fs.shaded.com.google.inject.internal.ProviderMethod$FastClassProviderMethod.doProvision(ProviderMethod.java:260)
at com.amazon.ws.emr.hadoop.fs.shaded.com.google.inject.internal.ProviderMethod.doProvision(ProviderMethod.java:171)
at com.amazon.ws.emr.hadoop.fs.shaded.com.google.inject.internal.InternalProviderInstanceBindingImpl$CyclicFactory.provision(InternalProviderInstanceBindingImpl.java:185)
at com.amazon.ws.emr.hadoop.fs.shaded.com.google.inject.internal.InternalProviderInstanceBindingImpl$CyclicFactory.get(InternalProviderInstanceBindingImpl.java:162)
at com.amazon.ws.emr.hadoop.fs.shaded.com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40)
at com.amazon.ws.emr.hadoop.fs.shaded.com.google.inject.internal.SingletonScope$1."
1740517632834,"get(SingletonScope.java:169)
at com.amazon.ws.emr.hadoop.fs.shaded.com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
at com.amazon.ws.emr.hadoop.fs.shaded.com.google.inject.internal.SingleParameterInjector.inject(SingleParameterInjector.java:40)
at com.amazon.ws.emr.hadoop.fs.shaded.com.google.inject.internal.SingleParameterInjector.getAll(SingleParameterInjector.java:60)
at com.amazon.ws.emr.hadoop.fs.shaded.com.google.inject.internal.ProviderMethod.doProvision(ProviderMethod.java:171)
at com.amazon.ws.emr.hadoop.fs.shaded.com.google.inject.internal.InternalProviderInstanceBindingImpl$CyclicFactory.provision(InternalProviderInstanceBindingImpl.java:185)
at com.amazon.ws.emr.hadoop.fs.shaded.com.google.inject.internal.InternalProviderInstanceBindingImpl$CyclicFactory.get(InternalProviderInstanceBindingImpl.java:162)
at com.amazon.ws.emr.hadoop.fs.shaded.com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:"
1740517632834,"40)
at com.amazon.ws.emr.hadoop.fs.shaded.com.google.inject.internal.SingletonScope$1.get(SingletonScope.java:169)
at com.amazon.ws.emr.hadoop.fs.shaded.com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
at com.amazon.ws.emr.hadoop.fs.shaded.com.google.inject.internal.SingleFieldInjector.inject(SingleFieldInjector.java:50)
at com.amazon.ws.emr.hadoop.fs.shaded.com.google.inject.internal.MembersInjectorImpl.injectMembers(MembersInjectorImpl.java:146)
at com.amazon.ws.emr.hadoop.fs.shaded.com.google.inject.internal.ConstructorInjector.provision(ConstructorInjector.java:124)
at com.amazon.ws.emr.hadoop.fs.shaded.com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:91)
at com.amazon.ws.emr.hadoop.fs.shaded.com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:300)
at com.amazon.ws.emr.hadoop.fs.shaded.com.google.inject.internal.FactoryProxy.get(FactoryProxy.java:60)
at com.amazon.ws.emr.hadoop.f"
1740517632834,"s.shaded.com.google.inject.internal.InjectorImpl$1.get(InjectorImpl.java:1101)
at com.amazon.ws.emr.hadoop.fs.shaded.com.google.inject.internal.InjectorImpl.getInstance(InjectorImpl.java:1134)
at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:95)
at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3662)
at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:171)
at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3763)
at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3714)
at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:564)
at org.apache.hadoop.fs.Path.getFileSystem(Path.java:374)
at com.amazonaws.services.glue.LogPusher.<init>(LogPusher.scala:25)
at com.amazonaws.services.glue.ProcessLauncher.$anonfun$logPusher$3(ProcessLauncher.scala:259)
at scala.Option.map(Option.scala:230)
at com.amazonaws.services.glue.ProcessLauncher.$anonfun$logPusher$2(ProcessLauncher.scala:225)
at scala.Option$WithFilter.flatMap(Option.scala:271)
at"
1740517632834," com.amazonaws.services.glue.ProcessLauncher.<init>(ProcessLauncher.scala:216)
at com.amazonaws.services.glue.ProcessLauncher.<init>(ProcessLauncher.scala:197)
at com.amazonaws.services.glue.ProcessLauncher$.main(ProcessLauncher.scala:32)
at com.amazonaws.services.glue.ProcessLauncher.main(ProcessLauncher.scala)
"
1740517633102,"INFO	2025-02-25T21:07:13,102	3277	com.amazon.ws.emr.hadoop.fs.guice.DefaultAWSCredentialsProviderFactory	[main]	72	Unable to create provider using constructor: DefaultAWSCredentialsProviderChain(java.net.URI, org.apache.hadoop.conf.Configuration)
"
1740517633234,"INFO	2025-02-25T21:07:13,234	3409	com.amazon.ws.emr.hadoop.fs.util.ClientConfigurationFactory	[main]	85	Set initial getObject socket timeout to 2000 ms.
"
1740517633501,"INFO	2025-02-25T21:07:13,501	3676	com.amazonaws.services.glue.SafeLogging	[main]	60	Initializing logging subsystem
"
1740517639226,"INFO	2025-02-25T21:07:19,226	9401	org.apache.spark.emr.EMRParamSideChannel	[Thread-7]	177	Setting FGAC mode to false
"
1740517639237,"INFO	2025-02-25T21:07:19,237	9412	org.apache.spark.SparkContext	[Thread-7]	60	Running Spark version 3.5.2-amzn-1
"
1740517639237,"INFO	2025-02-25T21:07:19,237	9412	org.apache.spark.SparkContext	[Thread-7]	60	OS info Linux, 5.10.233-224.894.amzn2.x86_64, amd64
"
1740517639238,"INFO	2025-02-25T21:07:19,238	9413	org.apache.spark.SparkContext	[Thread-7]	60	Java version 17.0.13
"
1740517639269,"INFO	2025-02-25T21:07:19,268	9443	org.apache.spark.resource.ResourceUtils	[Thread-7]	60	==============================================================
"
1740517639269,"INFO	2025-02-25T21:07:19,269	9444	org.apache.spark.resource.ResourceUtils	[Thread-7]	60	No custom resources configured for spark.driver.
"
1740517639270,"INFO	2025-02-25T21:07:19,269	9444	org.apache.spark.resource.ResourceUtils	[Thread-7]	60	==============================================================
"
1740517639270,"INFO	2025-02-25T21:07:19,270	9445	org.apache.spark.SparkContext	[Thread-7]	60	Submitted application: nativespark-Hackathon-Test-Glue-2-jr_7db86166969a95536b1eda3fa68973f64cc35756c4337acebab08c9031857df5
"
1740517639295,"INFO	2025-02-25T21:07:19,294	9469	org.apache.spark.resource.ResourceProfile	[Thread-7]	60	Default ResourceProfile created, executor resources: Map(executorType -> name: executorType, amount: 1, script: , vendor: , cores -> name: cores, amount: 8, script: , vendor: , memory -> name: memory, amount: 20480, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
"
1740517639305,"INFO	2025-02-25T21:07:19,304	9479	org.apache.spark.resource.ResourceProfile	[Thread-7]	60	Limiting resource is cpus at 8 tasks per executor
"
1740517639306,"INFO	2025-02-25T21:07:19,306	9481	org.apache.spark.resource.ResourceProfileManager	[Thread-7]	60	Added ResourceProfile id: 0
"
1740517639311,"INFO	2025-02-25T21:07:19,311	9486	org.apache.spark.resource.ResourceProfile	[Thread-7]	60	User executor ResourceProfile created, executor resources: Map(executorType -> name: executorType, amount: 1, script: , vendor: , cores -> name: cores, amount: 8, script: , vendor: , memory -> name: memory, amount: 20480, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
"
1740517639312,"INFO	2025-02-25T21:07:19,312	9487	org.apache.spark.resource.ResourceProfile	[Thread-7]	60	Limiting resource is cpus at 8 tasks per executor
"
1740517639312,"INFO	2025-02-25T21:07:19,312	9487	org.apache.spark.resource.ResourceProfileManager	[Thread-7]	60	Added ResourceProfile id: 1
"
1740517639430,"INFO	2025-02-25T21:07:19,430	9605	org.apache.spark.SecurityManager	[Thread-7]	60	Changing view acls to: hadoop
"
1740517639431,"INFO	2025-02-25T21:07:19,430	9605	org.apache.spark.SecurityManager	[Thread-7]	60	Changing modify acls to: hadoop
"
1740517639431,"INFO	2025-02-25T21:07:19,431	9606	org.apache.spark.SecurityManager	[Thread-7]	60	Changing view acls groups to: 
"
1740517639432,"INFO	2025-02-25T21:07:19,431	9606	org.apache.spark.SecurityManager	[Thread-7]	60	Changing modify acls groups to: 
"
1740517639432,"INFO	2025-02-25T21:07:19,432	9607	org.apache.spark.SecurityManager	[Thread-7]	60	SecurityManager: authentication enabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
"
1740517639730,"INFO	2025-02-25T21:07:19,729	9904	org.apache.spark.util.Utils	[Thread-7]	60	Successfully started service 'sparkDriver' on port 43701.
"
1740517639781,"INFO	2025-02-25T21:07:19,781	9956	org.apache.spark.SparkEnv	[Thread-7]	60	Registering MapOutputTracker
"
1740517639848,"INFO	2025-02-25T21:07:19,848	10023	org.apache.spark.SparkEnv	[Thread-7]	60	Registering BlockManagerMaster
"
1740517639877,"INFO	2025-02-25T21:07:19,876	10051	org.apache.spark.storage.BlockManagerMasterEndpoint	[Thread-7]	60	Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
"
1740517639877,"INFO	2025-02-25T21:07:19,877	10052	org.apache.spark.storage.BlockManagerMasterEndpoint	[Thread-7]	60	BlockManagerMasterEndpoint up
"
1740517639885,"INFO	2025-02-25T21:07:19,885	10060	org.apache.spark.SparkEnv	[Thread-7]	60	Registering BlockManagerMasterHeartbeat
"
1740517639912,"INFO	2025-02-25T21:07:19,912	10087	org.apache.spark.storage.DiskBlockManager	[Thread-7]	60	Created local directory at /tmp/blockmgr-867bd0f9-967d-4127-b350-55be23dadaa3
"
1740517639930,"INFO	2025-02-25T21:07:19,930	10105	org.apache.spark.storage.memory.MemoryStore	[Thread-7]	60	MemoryStore started with capacity 11.8 GiB
"
1740517639947,"INFO	2025-02-25T21:07:19,947	10122	org.apache.spark.SparkEnv	[Thread-7]	60	Registering OutputCommitCoordinator
"
1740517639953,"INFO	2025-02-25T21:07:19,953	10128	org.apache.spark.subresultcache.SubResultCacheManager	[Thread-7]	60	Sub-result caches are disabled.
"
1740517639989,"INFO	2025-02-25T21:07:19,988	10163	org.apache.spark.SparkContext	[Thread-7]	60	Added JAR /tmp/glue-job-10039463435112849203/jars/1ZSjlv-AwsGlueMLLibs.jar at spark://172.39.96.161:43701/jars/1ZSjlv-AwsGlueMLLibs.jar with timestamp 1740517639227
"
1740517639990,"INFO	2025-02-25T21:07:19,990	10165	org.apache.spark.SparkContext	[Thread-7]	60	Added JAR /tmp/glue-job-10039463435112849203/jars/UcyzHl-aws-glue-di-package-5.0.349.jar at spark://172.39.96.161:43701/jars/UcyzHl-aws-glue-di-package-5.0.349.jar with timestamp 1740517639227
"
1740517640073,"INFO	2025-02-25T21:07:20,073	10248	org.apache.spark.SparkContext	[Thread-7]	60	Added archive /tmp/glue-job-10039463435112849203_glue_venv.zip#python_environment at spark://172.39.96.161:43701/files/glue-job-10039463435112849203_glue_venv.zip with timestamp 1740517639227
"
1740517640074,"INFO	2025-02-25T21:07:20,074	10249	org.apache.spark.util.Utils	[Thread-7]	60	Copying /tmp/glue-job-10039463435112849203_glue_venv.zip to /tmp/spark-ad6b0bc8-8d90-4e3b-a961-cd7c4c237d6b/glue-job-10039463435112849203_glue_venv.zip
"
1740517640096,"INFO	2025-02-25T21:07:20,096	10271	org.apache.spark.SparkContext	[Thread-7]	60	Unpacking an archive /tmp/glue-job-10039463435112849203_glue_venv.zip#python_environment from /tmp/spark-ad6b0bc8-8d90-4e3b-a961-cd7c4c237d6b/glue-job-10039463435112849203_glue_venv.zip to /tmp/spark-17ba951d-9c13-4cdd-9fad-aaf1131d1bc7/userFiles-ca85fb84-ac7e-440c-bf14-99577c0bd5f4/python_environment
"
1740517640594,"INFO	2025-02-25T21:07:20,594	10769	org.apache.spark.scheduler.cluster.glue.JESClusterManager	[Thread-7]	121	Python path for executors: python_environment
"
1740517640598,"INFO	2025-02-25T21:07:20,598	10773	org.apache.spark.deploy.glue.client.GlueRMClientFactory	[Thread-7]	60	JESClusterManager: Initializing JES client with endpoint: https://glue-jes-prod.us-east-1.amazonaws.com
"
1740517640647,"SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
"
1740517641528,"INFO	2025-02-25T21:07:21,527	11702	org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend	[Thread-7]	60	JESSchedulerBackend
"
1740517641531,"INFO	2025-02-25T21:07:21,530	11705	org.apache.spark.util.Utils	[Thread-7]	60	Using initial executors = 1, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances
"
1740517641578,"INFO	2025-02-25T21:07:21,578	11753	org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend	[Thread-7]	60	Starting JES Scheduler Backend.
"
1740517641580,"INFO	2025-02-25T21:07:21,580	11755	org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend	[Thread-7]	60	Set total expected executors: rpId: 0, numExecs: 1
"
1740517641581,"INFO	2025-02-25T21:07:21,580	11755	org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend	[Thread-7]	60	Initial executors are 1
"
1740517641586,"INFO	2025-02-25T21:07:21,586	11761	org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend	[executorAllocator]	60	Creating executors task for application spark-application-1740517641520 with resource profile 0
"
1740517641589,"INFO	2025-02-25T21:07:21,588	11763	org.apache.spark.scheduler.cluster.glue.allocator.ExecutorTaskAllocator	[executorAllocator]	60	Creating new task with arguments List(--driver-url, spark://CoarseGrainedScheduler@172.39.96.161:43701, --executor-id, 1, --app-id, spark-application-1740517641520, --cores, 8, --resourceProfileId, 0)
"
1740517641591,"INFO	2025-02-25T21:07:21,591	11766	org.apache.spark.deploy.glue.client.GlueTaskGroupRMClient	[executorAllocator]	60	creating executor task for executor 1; clientToken gr_tg-1a1061c7fe7ceae0e1a5e4ffbb785f586f78455f_e_1_a_spark-application-1740517641520_p_1
"
1740517641594,"INFO	2025-02-25T21:07:21,593	11768	org.apache.spark.util.Utils	[Thread-7]	60	Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39515.
"
1740517641594,"INFO	2025-02-25T21:07:21,594	11769	org.apache.spark.network.netty.NettyBlockTransferService	[Thread-7]	88	Server created on 172.39.96.161:39515
"
1740517641596,"INFO	2025-02-25T21:07:21,596	11771	org.apache.spark.storage.BlockManager	[Thread-7]	60	Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
"
1740517641603,"INFO	2025-02-25T21:07:21,603	11778	org.apache.spark.storage.BlockManagerMaster	[Thread-7]	60	Registering BlockManager BlockManagerId(driver, 172.39.96.161, 39515, None)
"
1740517641608,"INFO	2025-02-25T21:07:21,608	11783	org.apache.spark.deploy.glue.client.GlueTaskGroupRMClient	[executorAllocator]	60	Sending request to JES
"
1740517641610,"INFO	2025-02-25T21:07:21,610	11785	org.apache.spark.storage.BlockManagerMasterEndpoint	[dispatcher-BlockManagerMaster]	60	Registering block manager 172.39.96.161:39515 with 11.8 GiB RAM, BlockManagerId(driver, 172.39.96.161, 39515, None)
"
1740517641614,"INFO	2025-02-25T21:07:21,613	11788	org.apache.spark.storage.BlockManagerMaster	[Thread-7]	60	Registered BlockManager BlockManagerId(driver, 172.39.96.161, 39515, None)
"
1740517641614,"INFO	2025-02-25T21:07:21,614	11789	org.apache.spark.storage.BlockManager	[Thread-7]	60	Initialized BlockManager: BlockManagerId(driver, 172.39.96.161, 39515, None)
"
1740517641913,"INFO	2025-02-25T21:07:21,913	12088	org.apache.spark.metrics.sink.GlueCloudwatchSink	[Thread-7]	16	CloudwatchSink: jobName: Hackathon-Test-Glue-2 jobRunId: jr_7db86166969a95536b1eda3fa68973f64cc35756c4337acebab08c9031857df5
"
1740517642127,"INFO	2025-02-25T21:07:22,126	12301	org.apache.spark.deploy.history.SingleEventLogFileWriter	[Thread-7]	60	Logging events to file:/var/log/spark/apps/spark-application-1740517641520.inprogress
"
1740517642254,"INFO	2025-02-25T21:07:22,253	12428	org.apache.spark.util.Utils	[Thread-7]	60	Using initial executors = 1, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances
"
1740517642255,"INFO	2025-02-25T21:07:22,254	12429	org.apache.spark.ExecutorAllocationManager	[Thread-7]	60	Dynamic allocation is enabled without a shuffle service.
"
1740517642271,"INFO	2025-02-25T21:07:22,271	12446	org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend	[Thread-7]	60	Set total expected executors: rpId: 0, numExecs: 1
"
1740517642271,"INFO	2025-02-25T21:07:22,271	12446	org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend	[Thread-7]	60	Requested total executors are 1
"
1740517642319,"INFO	2025-02-25T21:07:22,319	12494	com.amazonaws.services.glueexceptionanalysis.EventLogFileWriter	[Thread-7]	51	Started file writer for com.amazonaws.services.glueexceptionanalysis.EventLogFileWriter
"
1740517642330,"INFO	2025-02-25T21:07:22,330	12505	org.apache.spark.SparkContext	[Thread-7]	60	Registered listener com.amazonaws.services.glueexceptionanalysis.GlueExceptionAnalysisListener
"
1740517642346,"INFO	2025-02-25T21:07:22,345	12520	org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend	[Thread-7]	60	getDriverLogUrls Map()
"
1740517642352,"INFO	2025-02-25T21:07:22,352	12527	org.apache.spark.executor.ExecutorLogUrlHandler	[Thread-7]	60	Fail to renew executor log urls: some of required attributes are missing in app's event log.. Required: Set(CONTAINER_ID) / available: Set(). Falling back to show app's original log urls.
"
1740517642355,"INFO	2025-02-25T21:07:22,354	12529	org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend	[Thread-7]	60	SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
"
1740517642441,"INFO	2025-02-25T21:07:22,440	12615	org.apache.spark.deploy.glue.client.GlueTaskGroupRMClient	[executorAllocator]	60	createChildTask API response code 200
"
1740517642442,"INFO	2025-02-25T21:07:22,441	12616	org.apache.spark.scheduler.cluster.glue.allocator.ExecutorTaskAllocator	[executorAllocator]	60	executor task g-9fcd4d9979d88289cb60e4e5e32e1ace7e3a44bf created for executor 1 in resource profile 0
"
1740517642997,"INFO	2025-02-25T21:07:22,996	13171	com.amazonaws.services.glue.GlueContext	[Thread-7]	119	GlueMetrics configured and enabled
"
1740517642999,"INFO	2025-02-25T21:07:22,999	13174	org.apache.spark.metrics.source.ObservabilityTaskInfoRecorderListener	[Thread-7]	28	ThroughputMetricsSource is initiated
"
1740517643011,"INFO	2025-02-25T21:07:23,011	13186	org.apache.spark.metrics.source.ObservabilityTaskInfoRecorderListener	[Thread-7]	53	ResourceUtilizationMetricsSource is initiated
"
1740517643014,"INFO	2025-02-25T21:07:23,014	13189	org.apache.spark.metrics.source.StageSkewness	[Thread-7]	29	[Observability] Skewness metric using Skewness Factor = 5
"
1740517643016,"INFO	2025-02-25T21:07:23,015	13190	org.apache.spark.metrics.source.ObservabilityTaskInfoRecorderListener	[Thread-7]	68	PerformanceMetricsSource is initiated
"
1740517643016,"INFO	2025-02-25T21:07:23,016	13191	com.amazonaws.services.glue.GlueContext	[Thread-7]	127	ObservabilityMetrics configured and enabled
"
1740517643029,"INFO	2025-02-25T21:07:23,029	13204	com.amazonaws.services.glue.utils.EndpointConfig$	[Thread-7]	36	 STAGE is prod
"
1740517643031,"INFO	2025-02-25T21:07:23,030	13205	com.amazonaws.services.glue.utils.EndpointConfig$	[Thread-7]	90	Endpoints: {credentials_provider=com.amazonaws.auth.DefaultAWSCredentialsProviderChain, glue.endpoint=https://glue.us-east-1.amazonaws.com, lakeformation.endpoint=https://lakeformation.us-east-1.amazonaws.com, jes.endpoint=https://glue-jes-prod.us-east-1.amazonaws.com, region=us-east-1}
"
1740517643082,"INFO	2025-02-25T21:07:23,082	13257	com.amazonaws.services.glue.util.Job$	[Thread-7]	94	runId Method is Invoked 
"
1740517643083,"INFO	2025-02-25T21:07:23,082	13257	com.amazonaws.services.glue.util.Job$	[Thread-7]	103	Job run ID under runId method is jr_7db86166969a95536b1eda3fa68973f64cc35756c4337acebab08c9031857df5 
"
1740517643109,"INFO	2025-02-25T21:07:23,109	13284	com.amazon.ws.emr.hadoop.fs.guice.DefaultAWSCredentialsProviderFactory	[Thread-7]	72	Unable to create provider using constructor: DefaultAWSCredentialsProviderChain(java.net.URI, org.apache.hadoop.conf.Configuration)
"
1740517643112,"INFO	2025-02-25T21:07:23,110	13285	com.amazon.ws.emr.hadoop.fs.util.ClientConfigurationFactory	[Thread-7]	85	Set initial getObject socket timeout to 2000 ms.
"
1740517643112,"INFO	2025-02-25T21:07:23,112	13287	com.amazonaws.services.glue.util.FileListPersistence	[Thread-7]	37	create FileListPersistence with conf: fs.s3.serverSideEncryption.kms.keyId: None
"
1740517643113,"INFO	2025-02-25T21:07:23,113	13288	com.amazonaws.services.glue.utils.EndpointConfig$	[Thread-7]	36	 STAGE is prod
"
1740517643117,"INFO	2025-02-25T21:07:23,117	13292	com.amazonaws.services.glue.utils.EndpointConfig$	[Thread-7]	90	Endpoints: {credentials_provider=com.amazonaws.auth.DefaultAWSCredentialsProviderChain, glue.endpoint=https://glue.us-east-1.amazonaws.com, lakeformation.endpoint=https://lakeformation.us-east-1.amazonaws.com, jes.endpoint=https://glue-jes-prod.us-east-1.amazonaws.com, region=us-east-1}
"
1740517643163,"INFO	2025-02-25T21:07:23,163	13338	com.amazonaws.services.glue.util.AvroReaderUtil$	[Thread-7]	245	Creating default Avro field parser for version 1.7.
"
1740517643341,"INFO	2025-02-25T21:07:23,341	13516	org.apache.spark.storage.memory.MemoryStore	[Thread-7]	60	Block broadcast_0 stored as values in memory (estimated size 242.9 KiB, free 11.8 GiB)
"
1740517643450,"INFO	2025-02-25T21:07:23,450	13625	org.apache.spark.storage.memory.MemoryStore	[Thread-7]	60	Block broadcast_0_piece0 stored as bytes in memory (estimated size 40.7 KiB, actual size: 40.7 KiB, free 11.8 GiB)
"
1740517643453,"INFO	2025-02-25T21:07:23,453	13628	org.apache.spark.storage.BlockManagerInfo	[dispatcher-BlockManagerMaster]	60	Added broadcast_0_piece0 in memory on 172.39.96.161:39515 (size: 40.7 KiB, free: 11.8 GiB)
"
1740517643459,"INFO	2025-02-25T21:07:23,458	13633	org.apache.spark.SparkContext	[Thread-7]	60	Created broadcast 0 from broadcast at DynamoConnection.scala:55
"
1740517643473,"INFO	2025-02-25T21:07:23,473	13648	org.apache.spark.storage.memory.MemoryStore	[Thread-7]	60	Block broadcast_1 stored as values in memory (estimated size 242.9 KiB, free 11.8 GiB)
"
1740517643497,"INFO	2025-02-25T21:07:23,496	13671	org.apache.spark.storage.memory.MemoryStore	[Thread-7]	60	Block broadcast_1_piece0 stored as bytes in memory (estimated size 40.7 KiB, actual size: 40.7 KiB, free 11.8 GiB)
"
1740517643498,"INFO	2025-02-25T21:07:23,497	13672	org.apache.spark.storage.BlockManagerInfo	[dispatcher-BlockManagerMaster]	60	Added broadcast_1_piece0 in memory on 172.39.96.161:39515 (size: 40.7 KiB, free: 11.8 GiB)
"
1740517643499,"INFO	2025-02-25T21:07:23,499	13674	org.apache.spark.SparkContext	[Thread-7]	60	Created broadcast 1 from broadcast at DynamoConnection.scala:55
"
1740517643501,"INFO	2025-02-25T21:07:23,500	13675	com.amazonaws.services.glue.util.JobBookmark$	[Thread-7]	71	jobbookmark is not enabled, do not init AWSGlueJobBookMarkService
"
1740517643527,"INFO	2025-02-25T21:07:23,527	13702	org.apache.spark.sql.internal.SharedState	[Thread-7]	60	spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
"
1740517643530,"INFO	2025-02-25T21:07:23,530	13705	org.apache.spark.sql.internal.SharedState	[Thread-7]	60	Warehouse path is 'file:/tmp/spark-warehouse'.
"
1740517645227,"INFO	2025-02-25T21:07:25,226	15401	org.apache.spark.storage.BlockManagerInfo	[dispatcher-BlockManagerMaster]	60	Removed broadcast_0_piece0 on 172.39.96.161:39515 in memory (size: 40.7 KiB, free: 11.8 GiB)
"
1740517645232,"INFO	2025-02-25T21:07:25,231	15406	org.apache.spark.storage.BlockManagerInfo	[dispatcher-BlockManagerMaster]	60	Removed broadcast_1_piece0 on 172.39.96.161:39515 in memory (size: 40.7 KiB, free: 11.8 GiB)
"
1740517645311,"INFO	2025-02-25T21:07:25,310	15485	org.opensearch.hadoop.util.Version	[Thread-7]	143	OpenSearch Hadoop v1.2.0 [2a4148055f]
"
1740517645645,"INFO	2025-02-25T21:07:25,645	15820	com.amazon.ws.emr.hadoop.fs.guice.DefaultAWSCredentialsProviderFactory	[Thread-7]	72	Unable to create provider using constructor: DefaultAWSCredentialsProviderChain(java.net.URI, org.apache.hadoop.conf.Configuration)
"
1740517645646,"INFO	2025-02-25T21:07:25,646	15821	com.amazon.ws.emr.hadoop.fs.util.ClientConfigurationFactory	[Thread-7]	85	Set initial getObject socket timeout to 2000 ms.
"
1740517646258,"INFO	2025-02-25T21:07:26,257	16432	org.apache.spark.sql.execution.datasources.InMemoryFileIndex	[Thread-7]	60	It took 26 ms to list leaf files for 1 paths.
"
1740517646321,"INFO	2025-02-25T21:07:26,321	16496	org.apache.spark.sql.execution.datasources.InMemoryFileIndex	[Thread-7]	60	It took 2 ms to list leaf files for 1 paths.
"
1740517649269,"INFO	2025-02-25T21:07:29,268	19443	org.apache.spark.sql.execution.datasources.FileSourceStrategy	[Thread-7]	60	Pushed Filters: 
"
1740517649270,"INFO	2025-02-25T21:07:29,269	19444	org.apache.spark.sql.execution.datasources.FileSourceStrategy	[Thread-7]	60	Post-Scan Filters: (length(trim(value#0, None)) > 0)
"
1740517649523,"INFO	2025-02-25T21:07:29,522	19697	org.apache.spark.storage.memory.MemoryStore	[Thread-7]	60	Block broadcast_2 stored as values in memory (estimated size 292.5 KiB, free 11.8 GiB)
"
1740517649545,"INFO	2025-02-25T21:07:29,544	19719	org.apache.spark.storage.memory.MemoryStore	[Thread-7]	60	Block broadcast_2_piece0 stored as bytes in memory (estimated size 55.4 KiB, actual size: 55.4 KiB, free 11.8 GiB)
"
1740517649546,"INFO	2025-02-25T21:07:29,546	19721	org.apache.spark.storage.BlockManagerInfo	[dispatcher-BlockManagerMaster]	60	Added broadcast_2_piece0 in memory on 172.39.96.161:39515 (size: 55.4 KiB, free: 11.8 GiB)
"
1740517649548,"INFO	2025-02-25T21:07:29,548	19723	org.apache.spark.SparkContext	[Thread-7]	60	Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
"
1740517649602,"INFO	2025-02-25T21:07:29,601	19776	com.hadoop.compression.lzo.GPLNativeCodeLoader	[Thread-7]	34	Loaded native gpl library
"
1740517649609,"INFO	2025-02-25T21:07:29,608	19783	com.hadoop.compression.lzo.LzoCodec	[Thread-7]	76	Successfully loaded & initialized native-lzo library [hadoop-lzo rev 049362b7cf53ff5f739d6b1532457f2c6cd495e8]
"
1740517649650,"INFO	2025-02-25T21:07:29,649	19824	org.apache.spark.sql.execution.FileSourceScanExec	[Thread-7]	60	Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
"
1740517649656,"INFO	2025-02-25T21:07:29,656	19831	org.apache.spark.sql.execution.FileSourceScanExec	[Thread-7]	60	relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
"
1740517649957,"INFO	2025-02-25T21:07:29,957	20132	org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator	[Thread-7]	60	Code generated in 247.007305 ms
"
1740517650155,"INFO	2025-02-25T21:07:30,155	20330	org.apache.spark.SparkContext	[Thread-7]	60	Starting job: csv at NativeMethodAccessorImpl.java:0
"
1740517650176,"INFO	2025-02-25T21:07:30,175	20350	org.apache.spark.scheduler.DAGScheduler	[dag-scheduler-event-loop]	60	Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
"
1740517650176,"INFO	2025-02-25T21:07:30,176	20351	org.apache.spark.scheduler.DAGScheduler	[dag-scheduler-event-loop]	60	Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
"
1740517650176,"INFO	2025-02-25T21:07:30,176	20351	org.apache.spark.scheduler.DAGScheduler	[dag-scheduler-event-loop]	60	Parents of final stage: List()
"
1740517650178,"INFO	2025-02-25T21:07:30,178	20353	org.apache.spark.scheduler.DAGScheduler	[dag-scheduler-event-loop]	60	Missing parents: List()
"
1740517650194,"INFO	2025-02-25T21:07:30,193	20368	org.apache.spark.scheduler.DAGScheduler	[dag-scheduler-event-loop]	60	Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
"
1740517650313,"INFO	2025-02-25T21:07:30,312	20487	org.apache.spark.storage.memory.MemoryStore	[dag-scheduler-event-loop]	60	Block broadcast_3 stored as values in memory (estimated size 16.9 KiB, free 11.8 GiB)
"
1740517650316,"INFO	2025-02-25T21:07:30,315	20490	org.apache.spark.storage.memory.MemoryStore	[dag-scheduler-event-loop]	60	Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.3 KiB, actual size: 8.3 KiB, free 11.8 GiB)
"
1740517650317,"INFO	2025-02-25T21:07:30,316	20491	org.apache.spark.storage.BlockManagerInfo	[dispatcher-BlockManagerMaster]	60	Added broadcast_3_piece0 in memory on 172.39.96.161:39515 (size: 8.3 KiB, free: 11.8 GiB)
"
1740517650318,"INFO	2025-02-25T21:07:30,318	20493	org.apache.spark.SparkContext	[dag-scheduler-event-loop]	60	Created broadcast 3 from broadcast at DAGScheduler.scala:1664
"
1740517650341,"INFO	2025-02-25T21:07:30,340	20515	org.apache.spark.scheduler.DAGScheduler	[dag-scheduler-event-loop]	60	Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
"
1740517650342,"INFO	2025-02-25T21:07:30,342	20517	org.apache.spark.scheduler.TaskSchedulerImpl	[dag-scheduler-event-loop]	60	Adding task set 0.0 with 1 tasks resource profile 0
"
1740517661673,"INFO	2025-02-25T21:07:41,673	31848	org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend$JESAsSchedulerBackendEndpoint	[dispatcher-CoarseGrainedScheduler]	60	Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.39.45.198:53932) with ID 1,  ResourceProfileId 0
"
1740517661675,"INFO	2025-02-25T21:07:41,674	31849	org.apache.spark.executor.ExecutorLogUrlHandler	[dispatcher-CoarseGrainedScheduler]	60	Fail to renew executor log urls: some of required attributes are missing in app's event log.. Required: Set(CONTAINER_ID) / available: Set(). Falling back to show app's original log urls.
"
1740517661676,"INFO	2025-02-25T21:07:41,675	31850	org.apache.spark.scheduler.cluster.glue.ExecutorEventListener	[spark-listener-group-shared]	60	Got executor added event for 1 @ 1740517661675
"
1740517661677,"INFO	2025-02-25T21:07:41,677	31852	org.apache.spark.scheduler.cluster.glue.allocator.ExecutorTaskAllocator	[spark-listener-group-shared]	60	connected executor 1
"
1740517661680,"INFO	2025-02-25T21:07:41,679	31854	org.apache.spark.scheduler.dynalloc.ExecutorMonitor	[spark-listener-group-executorManagement]	60	New executor 1 has registered (new total is 1)
"
1740517661743,"INFO	2025-02-25T21:07:41,743	31918	org.apache.spark.storage.BlockManagerMasterEndpoint	[dispatcher-BlockManagerMaster]	60	Registering block manager 172.39.45.198:38317 with 11.8 GiB RAM, BlockManagerId(1, 172.39.45.198, 38317, None)
"
1740517662988,"INFO	2025-02-25T21:07:42,988	33163	org.apache.spark.scheduler.TaskSetManager	[dispatcher-CoarseGrainedScheduler]	60	Starting task 0.0 in stage 0.0 (TID 0) (172.39.45.198, executor 1, partition 0, ANY, 10424 bytes) 
"
1740517663404,"INFO	2025-02-25T21:07:43,404	33579	org.apache.spark.storage.BlockManagerInfo	[dispatcher-BlockManagerMaster]	60	Added broadcast_3_piece0 in memory on 172.39.45.198:38317 (size: 8.3 KiB, free: 11.8 GiB)
"
1740517663479,"INFO	2025-02-25T21:07:43,478	33653	com.amazonaws.services.glue.LogPusher	[pool-6-thread-1]	60	uploading file:///var/log/spark/apps to s3://aws-glue-assets-842675997893-us-east-1/sparkHistoryLogs/
"
1740517663582,"ERROR	2025-02-25T21:07:43,578	33753	com.amazonaws.services.glue.LogPusher	[pool-6-thread-1]	97	com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: K11TCYK3MCKS8TXG; S3 Extended Request ID: gWDJg1g/1Szv06gCpjnV0rU8KZGTNc1sEIyAThJ/WyEIjY2SHpFtPdHZ/MuoSKl+3wvM6p2rthj+8CuC02kj3xHZjpmherIb1cWkHxqtGFg=; Proxy: null), S3 Extended Request ID: gWDJg1g/1Szv06gCpjnV0rU8KZGTNc1sEIyAThJ/WyEIjY2SHpFtPdHZ/MuoSKl+3wvM6p2rthj+8CuC02kj3xHZjpmherIb1cWkHxqtGFg=
java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: K11TCYK3MCKS8TXG; S3 Extended Request ID: gWDJg1g/1Szv06gCpjnV0rU8KZGTNc1sEIyAThJ/WyEIjY2SHpFtPdHZ/MuoSKl+3wvM6p2rthj+8CuC02kj3xHZjpmherIb1cWkHxqtGFg=; Proxy: null), S3 Extended Request ID: gWDJg1g/1Szv06gCpjnV0"
1740517663582,"rU8KZGTNc1sEIyAThJ/WyEIjY2SHpFtPdHZ/MuoSKl+3wvM6p2rthj+8CuC02kj3xHZjpmherIb1cWkHxqtGFg=
	at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:429) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolderUsingFolderObject(Jets3tNativeFileSystemStore.java:255) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolder(Jets3tNativeFileSystemStore.java:218) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:554) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at org.apache.hadoop.fs.FileUtil.checkDest(FileUtil.java:611) ~[hadoop-client-api-3.4.0-amzn-1.jar:?]
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:468) ~[hadoop-client-api-3.4.0-amzn-1.jar:?]
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:432) ~[hadoop-client-api-3.4.0-amzn-1.jar:?]
	at org.apache.hadoop.fs.FileSystem.copyF"
1740517663583,"romLocalFile(FileSystem.java:2647) ~[hadoop-client-api-3.4.0-amzn-1.jar:?]
	at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:2613) ~[hadoop-client-api-3.4.0-amzn-1.jar:?]
	at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:2575) ~[hadoop-client-api-3.4.0-amzn-1.jar:?]
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.copyFromLocalFile(EmrFileSystem.java:518) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazonaws.services.glue.LogPusher.upload(LogPusher.scala:72) [UcyzHl-aws-glue-di-package-5.0.349.jar:?]
	at com.amazonaws.services.glue.LogPusher.run(LogPusher.scala:93) [UcyzHl-aws-glue-di-package-5.0.349.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305) [?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	a"
1740517663583,"t java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: K11TCYK3MCKS8TXG; S3 Extended Request ID: gWDJg1g/1Szv06gCpjnV0rU8KZGTNc1sEIyAThJ/WyEIjY2SHpFtPdHZ/MuoSKl+3wvM6p2rthj+8CuC02kj3xHZjpmherIb1cWkHxqtGFg=; Proxy: null)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387) ~[emrfs-hadoop-"
1740517663583,"assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697) ~[emrfs-hadoop-assembl"
1740517663583,"y-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5558) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5505) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:1002) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:26) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:12) ~[emrfs-hadoop-ass"
1740517663583,"embly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor$CallPerformer.call(GlobalS3Executor.java:114) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:141) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:196) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:420) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	... 18 more
"
1740517665013,"INFO	2025-02-25T21:07:45,013	35188	org.apache.spark.storage.BlockManagerInfo	[dispatcher-BlockManagerMaster]	60	Added broadcast_2_piece0 in memory on 172.39.45.198:38317 (size: 55.4 KiB, free: 11.8 GiB)
"
1740517666173,"INFO	2025-02-25T21:07:46,173	36348	org.apache.spark.scheduler.TaskSetManager	[task-result-getter-0]	60	Finished task 0.0 in stage 0.0 (TID 0) in 3201 ms on 172.39.45.198 (executor 1) (1/1)
"
1740517666175,"INFO	2025-02-25T21:07:46,175	36350	org.apache.spark.scheduler.TaskSchedulerImpl	[task-result-getter-0]	60	Removed TaskSet 0.0, whose tasks have all completed, from pool 
"
1740517666180,"INFO	2025-02-25T21:07:46,180	36355	org.apache.spark.scheduler.DAGScheduler	[dag-scheduler-event-loop]	60	ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 15.970 s
"
1740517666185,"INFO	2025-02-25T21:07:46,185	36360	org.apache.spark.scheduler.DAGScheduler	[dag-scheduler-event-loop]	60	Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
"
1740517666186,"INFO	2025-02-25T21:07:46,185	36360	org.apache.spark.scheduler.TaskSchedulerImpl	[dag-scheduler-event-loop]	60	Killing all running tasks in stage 0: Stage finished
"
1740517666188,"INFO	2025-02-25T21:07:46,188	36363	org.apache.spark.scheduler.DAGScheduler	[Thread-7]	60	Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 16.032162 s
"
1740517666212,"INFO	2025-02-25T21:07:46,212	36387	org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator	[Thread-7]	60	Code generated in 13.679154 ms
"
1740517666226,"INFO	2025-02-25T21:07:46,226	36401	org.apache.spark.sql.execution.SQLExecution	[Thread-7]	60	Generating and posting SparkListenerSQLExecutionObfuscatedInfo...
"
1740517666229,"INFO	2025-02-25T21:07:46,229	36404	org.apache.spark.sql.execution.SQLExecution	[Thread-7]	60	Posted SparkListenerSQLExecutionObfuscatedInfo in 4 ms
"
1740517666286,"INFO	2025-02-25T21:07:46,286	36461	org.apache.spark.sql.execution.datasources.FileSourceStrategy	[Thread-7]	60	Pushed Filters: 
"
1740517666287,"INFO	2025-02-25T21:07:46,286	36461	org.apache.spark.sql.execution.datasources.FileSourceStrategy	[Thread-7]	60	Post-Scan Filters: 
"
1740517666294,"INFO	2025-02-25T21:07:46,293	36468	org.apache.spark.storage.memory.MemoryStore	[Thread-7]	60	Block broadcast_4 stored as values in memory (estimated size 292.5 KiB, free 11.8 GiB)
"
1740517666306,"INFO	2025-02-25T21:07:46,306	36481	org.apache.spark.storage.memory.MemoryStore	[Thread-7]	60	Block broadcast_4_piece0 stored as bytes in memory (estimated size 55.4 KiB, actual size: 55.4 KiB, free 11.8 GiB)
"
1740517666307,"INFO	2025-02-25T21:07:46,307	36482	org.apache.spark.storage.BlockManagerInfo	[dispatcher-BlockManagerMaster]	60	Added broadcast_4_piece0 in memory on 172.39.96.161:39515 (size: 55.4 KiB, free: 11.8 GiB)
"
1740517666308,"INFO	2025-02-25T21:07:46,307	36482	org.apache.spark.SparkContext	[Thread-7]	60	Created broadcast 4 from csv at NativeMethodAccessorImpl.java:0
"
1740517666309,"INFO	2025-02-25T21:07:46,309	36484	org.apache.spark.sql.execution.FileSourceScanExec	[Thread-7]	60	Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
"
1740517666309,"INFO	2025-02-25T21:07:46,309	36484	org.apache.spark.sql.execution.FileSourceScanExec	[Thread-7]	60	relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
"
1740517666624,"INFO	2025-02-25T21:07:46,624	36799	com.amazonaws.services.glue.ProcessLauncher	[main]	60	postprocessing
"
1740517666624,"INFO	2025-02-25T21:07:46,624	36799	com.amazonaws.services.glue.ProcessLauncher	[main]	633	Enhance failure reason and emit cloudwatch error metrics.
"
1740517666645,"INFO	2025-02-25T21:07:46,644	36819	com.amazonaws.services.glue.CloudWatchMetricsEmitter	[main]	34	Emit job error metrics
"
1740517666788,"INFO	2025-02-25T21:07:46,788	36963	com.amazonaws.services.glue.LogPusher	[main]	60	stopping
"
1740517666807,"INFO	2025-02-25T21:07:46,791	36966	org.apache.spark.SparkContext	[shutdown-hook-0]	60	Invoking stop() from shutdown hook
"
1740517666807,"INFO	2025-02-25T21:07:46,791	36966	org.apache.spark.SparkContext	[shutdown-hook-0]	60	SparkContext is stopping with exitCode 0.
"
1740517666807,"INFO	2025-02-25T21:07:46,795	36970	org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend	[shutdown-hook-0]	60	Stopping JES Scheduler Backend.
"
1740517666807,"INFO	2025-02-25T21:07:46,796	36971	org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend	[shutdown-hook-0]	60	Shutting down all executors
"
1740517666807,"INFO	2025-02-25T21:07:46,797	36972	org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend$JESAsSchedulerBackendEndpoint	[dispatcher-CoarseGrainedScheduler]	60	Asking each executor to shut down
"
1740517666807,"INFO	2025-02-25T21:07:46,799	36974	com.amazonaws.services.glueexceptionanalysis.EventLogFileWriter	[spark-listener-group-shared]	70	Logs, events processed and insights are written to file /tmp/glue-exception-analysis-logs/spark-application-1740517641520
"
1740517666808,"INFO	2025-02-25T21:07:46,807	36982	org.apache.spark.MapOutputTrackerMasterEndpoint	[dispatcher-event-loop-3]	60	MapOutputTrackerMasterEndpoint stopped!
"
1740517666817,"INFO	2025-02-25T21:07:46,817	36992	org.apache.spark.storage.memory.MemoryStore	[shutdown-hook-0]	60	MemoryStore cleared
"
1740517666818,"INFO	2025-02-25T21:07:46,817	36992	org.apache.spark.storage.BlockManager	[shutdown-hook-0]	60	BlockManager stopped
"
1740517666821,"INFO	2025-02-25T21:07:46,820	36995	org.apache.spark.storage.BlockManagerMaster	[shutdown-hook-0]	60	BlockManagerMaster stopped
"
1740517667224,"INFO	2025-02-25T21:07:47,224	37399	org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint	[dispatcher-event-loop-5]	60	OutputCommitCoordinator stopped!
"
1740517667268,"INFO	2025-02-25T21:07:47,268	37443	org.apache.spark.SparkContext	[shutdown-hook-0]	60	Successfully stopped SparkContext
"
1740517667269,"INFO	2025-02-25T21:07:47,269	37444	com.amazonaws.services.glue.LogPusher	[shutdown-hook-0]	60	uploading file:///var/log/spark/apps to s3://aws-glue-assets-842675997893-us-east-1/sparkHistoryLogs/
"
1740517667299,"ERROR	2025-02-25T21:07:47,299	37474	com.amazonaws.services.glue.LogPusher	[shutdown-hook-0]	97	com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: A5HY6W5V2PX5K2YK; S3 Extended Request ID: 6U5YQojbz8Lss6uKLsRUF3dntCmU57NVVH8B3rJfVy0IORZA2aNnb0ogtfaeNDiVRsY8s5UpfotnuV0NS5/7DqnnWsgHKS9pTyotAAzGABU=; Proxy: null), S3 Extended Request ID: 6U5YQojbz8Lss6uKLsRUF3dntCmU57NVVH8B3rJfVy0IORZA2aNnb0ogtfaeNDiVRsY8s5UpfotnuV0NS5/7DqnnWsgHKS9pTyotAAzGABU=
java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: A5HY6W5V2PX5K2YK; S3 Extended Request ID: 6U5YQojbz8Lss6uKLsRUF3dntCmU57NVVH8B3rJfVy0IORZA2aNnb0ogtfaeNDiVRsY8s5UpfotnuV0NS5/7DqnnWsgHKS9pTyotAAzGABU=; Proxy: null), S3 Extended Request ID: 6U5YQojbz8Lss6uKLsRUF"
1740517667300,"3dntCmU57NVVH8B3rJfVy0IORZA2aNnb0ogtfaeNDiVRsY8s5UpfotnuV0NS5/7DqnnWsgHKS9pTyotAAzGABU=
	at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:429) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolderUsingFolderObject(Jets3tNativeFileSystemStore.java:255) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolder(Jets3tNativeFileSystemStore.java:218) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:554) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at org.apache.hadoop.fs.FileUtil.checkDest(FileUtil.java:611) ~[hadoop-client-api-3.4.0-amzn-1.jar:?]
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:468) ~[hadoop-client-api-3.4.0-amzn-1.jar:?]
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:432) ~[hadoop-client-api-3.4.0-amzn-1.jar:?]
	at org.apache.hadoop.fs.FileSystem.copyF"
1740517667300,"romLocalFile(FileSystem.java:2647) ~[hadoop-client-api-3.4.0-amzn-1.jar:?]
	at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:2613) ~[hadoop-client-api-3.4.0-amzn-1.jar:?]
	at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:2575) ~[hadoop-client-api-3.4.0-amzn-1.jar:?]
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.copyFromLocalFile(EmrFileSystem.java:518) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazonaws.services.glue.LogPusher.upload(LogPusher.scala:72) ~[UcyzHl-aws-glue-di-package-5.0.349.jar:?]
	at org.apache.spark.util.ShutdownHookManagerWrapper$.$anonfun$addLogPusherHook$2(ShutdownHookManagerWrapper.scala:9) ~[UcyzHl-aws-glue-di-package-5.0.349.jar:3.5.2-amzn-1]
	at org.apache.spark.util.ShutdownHookManagerWrapper$.$anonfun$addLogPusherHook$2$adapted(ShutdownHookManagerWrapper.scala:9) ~[UcyzHl-aws-glue-di-package-5.0.349.jar:3.5.2-amzn-1]
	at scala.Option.foreach(Option.scala:407) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.util.ShutdownHookManagerWrapp"
1740517667300,"er$.$anonfun$addLogPusherHook$1(ShutdownHookManagerWrapper.scala:9) ~[UcyzHl-aws-glue-di-package-5.0.349.jar:3.5.2-amzn-1]
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214) ~[spark-core_2.12-3.5.2-amzn-1.jar:3.5.2-amzn-1]
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188) ~[spark-core_2.12-3.5.2-amzn-1.jar:3.5.2-amzn-1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1971) ~[spark-core_2.12-3.5.2-amzn-1.jar:3.5.2-amzn-1]
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188) ~[spark-core_2.12-3.5.2-amzn-1.jar:3.5.2-amzn-1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) [scala-library-2.12.18.jar:?]
	at scala.util.Try$.apply(Try.scala:213) [scala-library-2.12.18.jar:?]
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookM"
1740517667300,"anager.scala:188) [spark-core_2.12-3.5.2-amzn-1.jar:3.5.2-amzn-1]
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178) [spark-core_2.12-3.5.2-amzn-1.jar:3.5.2-amzn-1]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: A5HY6W5V2PX5K2YK; S3 Extended Request ID: 6U5YQojbz8Lss6uKLsRUF3dntCmU57NVVH8B3rJfVy0IORZA2aNnb0ogtfaeNDiVRsY8s5UpfotnuV0NS5/7DqnnWsgHKS9pTyotAAzGABU=; Proxy: null)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.Amazon"
1740517667300,"HttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com"
1740517667300,".amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5558) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5505)"
1740517667300," ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:1002) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:26) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:12) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor$CallPerformer.call(GlobalS3Executor.java:114) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:141) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:196) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191) ~[emrfs-hadoop-assembly-2.66.0"
1740517667300,".jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:420) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	... 29 more
"
1740517667300,"INFO	2025-02-25T21:07:47,300	37475	org.apache.spark.util.ShutdownHookManager	[shutdown-hook-0]	60	Shutdown hook called
"
1740517667300,"INFO	2025-02-25T21:07:47,300	37475	org.apache.spark.util.ShutdownHookManager	[shutdown-hook-0]	60	Deleting directory /tmp/spark-ad6b0bc8-8d90-4e3b-a961-cd7c4c237d6b
"
1740517667306,"INFO	2025-02-25T21:07:47,306	37481	org.apache.spark.util.ShutdownHookManager	[shutdown-hook-0]	60	Deleting directory /tmp/spark-17ba951d-9c13-4cdd-9fad-aaf1131d1bc7
"
1740517667310,"INFO	2025-02-25T21:07:47,310	37485	org.apache.spark.util.ShutdownHookManager	[shutdown-hook-0]	60	Deleting directory /tmp/spark-17ba951d-9c13-4cdd-9fad-aaf1131d1bc7/pyspark-1cc72030-127d-4d52-8954-04a02a921574
"