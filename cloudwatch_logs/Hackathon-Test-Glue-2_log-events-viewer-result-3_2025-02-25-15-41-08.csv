timestamp,message
1740516068522,"INFO	2025-02-25T20:41:08,522	3750	org.apache.spark.resource.ResourceUtils	[Thread-7]	60	No custom resources configured for spark.driver.
"
1740516068523,"INFO	2025-02-25T20:41:08,522	3750	org.apache.spark.resource.ResourceUtils	[Thread-7]	60	==============================================================
"
1740516068523,"INFO	2025-02-25T20:41:08,523	3751	org.apache.spark.SparkContext	[Thread-7]	60	Submitted application: nativespark-Hackathon-Test-Glue-2-jr_4b568db588200719ca859067253367b1f480062e908191c64a05ab0ab92739ae
"
1740516068549,"INFO	2025-02-25T20:41:08,548	3776	org.apache.spark.resource.ResourceProfile	[Thread-7]	60	Default ResourceProfile created, executor resources: Map(executorType -> name: executorType, amount: 1, script: , vendor: , cores -> name: cores, amount: 8, script: , vendor: , memory -> name: memory, amount: 20480, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
"
1740516068561,"INFO	2025-02-25T20:41:08,561	3789	org.apache.spark.resource.ResourceProfile	[Thread-7]	60	Limiting resource is cpus at 8 tasks per executor
"
1740516068564,"INFO	2025-02-25T20:41:08,563	3791	org.apache.spark.resource.ResourceProfileManager	[Thread-7]	60	Added ResourceProfile id: 0
"
1740516068567,"INFO	2025-02-25T20:41:08,566	3794	org.apache.spark.resource.ResourceProfile	[Thread-7]	60	User executor ResourceProfile created, executor resources: Map(executorType -> name: executorType, amount: 1, script: , vendor: , cores -> name: cores, amount: 8, script: , vendor: , memory -> name: memory, amount: 20480, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
"
1740516068567,"INFO	2025-02-25T20:41:08,567	3795	org.apache.spark.resource.ResourceProfile	[Thread-7]	60	Limiting resource is cpus at 8 tasks per executor
"
1740516068567,"INFO	2025-02-25T20:41:08,567	3795	org.apache.spark.resource.ResourceProfileManager	[Thread-7]	60	Added ResourceProfile id: 1
"
1740516068620,"INFO	2025-02-25T20:41:08,619	3847	org.apache.spark.SecurityManager	[Thread-7]	60	Changing view acls to: hadoop
"
1740516068620,"INFO	2025-02-25T20:41:08,620	3848	org.apache.spark.SecurityManager	[Thread-7]	60	Changing modify acls to: hadoop
"
1740516068620,"INFO	2025-02-25T20:41:08,620	3848	org.apache.spark.SecurityManager	[Thread-7]	60	Changing view acls groups to: 
"
1740516068621,"INFO	2025-02-25T20:41:08,621	3849	org.apache.spark.SecurityManager	[Thread-7]	60	Changing modify acls groups to: 
"
1740516068621,"INFO	2025-02-25T20:41:08,621	3849	org.apache.spark.SecurityManager	[Thread-7]	60	SecurityManager: authentication enabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
"
1740516068811,"INFO	2025-02-25T20:41:08,811	4039	org.apache.spark.util.Utils	[Thread-7]	60	Successfully started service 'sparkDriver' on port 34473.
"
1740516068842,"INFO	2025-02-25T20:41:08,841	4069	org.apache.spark.SparkEnv	[Thread-7]	60	Registering MapOutputTracker
"
1740516068883,"INFO	2025-02-25T20:41:08,882	4110	org.apache.spark.SparkEnv	[Thread-7]	60	Registering BlockManagerMaster
"
1740516068907,"INFO	2025-02-25T20:41:08,907	4135	org.apache.spark.storage.BlockManagerMasterEndpoint	[Thread-7]	60	Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
"
1740516068908,"INFO	2025-02-25T20:41:08,908	4136	org.apache.spark.storage.BlockManagerMasterEndpoint	[Thread-7]	60	BlockManagerMasterEndpoint up
"
1740516068912,"INFO	2025-02-25T20:41:08,912	4140	org.apache.spark.SparkEnv	[Thread-7]	60	Registering BlockManagerMasterHeartbeat
"
1740516068937,"INFO	2025-02-25T20:41:08,936	4164	org.apache.spark.storage.DiskBlockManager	[Thread-7]	60	Created local directory at /tmp/blockmgr-61915e65-1ffc-47ec-bc73-39c3f869a5bb
"
1740516068951,"INFO	2025-02-25T20:41:08,951	4179	org.apache.spark.storage.memory.MemoryStore	[Thread-7]	60	MemoryStore started with capacity 11.8 GiB
"
1740516068965,"INFO	2025-02-25T20:41:08,965	4193	org.apache.spark.SparkEnv	[Thread-7]	60	Registering OutputCommitCoordinator
"
1740516068970,"INFO	2025-02-25T20:41:08,969	4197	org.apache.spark.subresultcache.SubResultCacheManager	[Thread-7]	60	Sub-result caches are disabled.
"
1740516069001,"INFO	2025-02-25T20:41:09,001	4229	org.apache.spark.SparkContext	[Thread-7]	60	Added JAR /tmp/glue-job-15724319983310920015/jars/1ZSjlv-AwsGlueMLLibs.jar at spark://172.34.51.225:34473/jars/1ZSjlv-AwsGlueMLLibs.jar with timestamp 1740516068484
"
1740516069003,"INFO	2025-02-25T20:41:09,002	4230	org.apache.spark.SparkContext	[Thread-7]	60	Added JAR /tmp/glue-job-15724319983310920015/jars/UcyzHl-aws-glue-di-package-5.0.349.jar at spark://172.34.51.225:34473/jars/UcyzHl-aws-glue-di-package-5.0.349.jar with timestamp 1740516068484
"
1740516069037,"INFO	2025-02-25T20:41:09,036	4264	org.apache.spark.SparkContext	[Thread-7]	60	Added archive /tmp/glue-job-15724319983310920015_glue_venv.zip#python_environment at spark://172.34.51.225:34473/files/glue-job-15724319983310920015_glue_venv.zip with timestamp 1740516068484
"
1740516069039,"INFO	2025-02-25T20:41:09,038	4266	org.apache.spark.util.Utils	[Thread-7]	60	Copying /tmp/glue-job-15724319983310920015_glue_venv.zip to /tmp/spark-497ee5a4-ac26-4330-996c-d9b518a53f73/glue-job-15724319983310920015_glue_venv.zip
"
1740516069062,"INFO	2025-02-25T20:41:09,061	4289	org.apache.spark.SparkContext	[Thread-7]	60	Unpacking an archive /tmp/glue-job-15724319983310920015_glue_venv.zip#python_environment from /tmp/spark-497ee5a4-ac26-4330-996c-d9b518a53f73/glue-job-15724319983310920015_glue_venv.zip to /tmp/spark-e84c38a7-a043-47af-bf1a-bd1abb97139d/userFiles-1c777573-f796-4e82-9998-1a6792d77595/python_environment
"
1740516069523,"INFO	2025-02-25T20:41:09,523	4751	org.apache.spark.scheduler.cluster.glue.JESClusterManager	[Thread-7]	121	Python path for executors: python_environment
"
1740516069526,"INFO	2025-02-25T20:41:09,526	4754	org.apache.spark.deploy.glue.client.GlueRMClientFactory	[Thread-7]	60	JESClusterManager: Initializing JES client with endpoint: https://glue-jes-prod.us-east-1.amazonaws.com
"
1740516069546,"SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
"
1740516070197,"INFO	2025-02-25T20:41:10,197	5425	org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend	[Thread-7]	60	JESSchedulerBackend
"
1740516070200,"INFO	2025-02-25T20:41:10,199	5427	org.apache.spark.util.Utils	[Thread-7]	60	Using initial executors = 1, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances
"
1740516070244,"INFO	2025-02-25T20:41:10,244	5472	org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend	[Thread-7]	60	Starting JES Scheduler Backend.
"
1740516070246,"INFO	2025-02-25T20:41:10,246	5474	org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend	[Thread-7]	60	Set total expected executors: rpId: 0, numExecs: 1
"
1740516070247,"INFO	2025-02-25T20:41:10,247	5475	org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend	[Thread-7]	60	Initial executors are 1
"
1740516070253,"INFO	2025-02-25T20:41:10,253	5481	org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend	[executorAllocator]	60	Creating executors task for application spark-application-1740516070192 with resource profile 0
"
1740516070255,"INFO	2025-02-25T20:41:10,255	5483	org.apache.spark.util.Utils	[Thread-7]	60	Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44077.
"
1740516070256,"INFO	2025-02-25T20:41:10,255	5483	org.apache.spark.network.netty.NettyBlockTransferService	[Thread-7]	88	Server created on 172.34.51.225:44077
"
1740516070257,"INFO	2025-02-25T20:41:10,256	5484	org.apache.spark.scheduler.cluster.glue.allocator.ExecutorTaskAllocator	[executorAllocator]	60	Creating new task with arguments List(--driver-url, spark://CoarseGrainedScheduler@172.34.51.225:34473, --executor-id, 1, --app-id, spark-application-1740516070192, --cores, 8, --resourceProfileId, 0)
"
1740516070257,"INFO	2025-02-25T20:41:10,257	5485	org.apache.spark.storage.BlockManager	[Thread-7]	60	Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
"
1740516070259,"INFO	2025-02-25T20:41:10,259	5487	org.apache.spark.deploy.glue.client.GlueTaskGroupRMClient	[executorAllocator]	60	creating executor task for executor 1; clientToken gr_tg-44b684029e03db1c768d2ac6ae498bdd17326224_e_1_a_spark-application-1740516070192_p_1
"
1740516070265,"INFO	2025-02-25T20:41:10,265	5493	org.apache.spark.storage.BlockManagerMaster	[Thread-7]	60	Registering BlockManager BlockManagerId(driver, 172.34.51.225, 44077, None)
"
1740516070268,"INFO	2025-02-25T20:41:10,268	5496	org.apache.spark.storage.BlockManagerMasterEndpoint	[dispatcher-BlockManagerMaster]	60	Registering block manager 172.34.51.225:44077 with 11.8 GiB RAM, BlockManagerId(driver, 172.34.51.225, 44077, None)
"
1740516070271,"INFO	2025-02-25T20:41:10,270	5498	org.apache.spark.storage.BlockManagerMaster	[Thread-7]	60	Registered BlockManager BlockManagerId(driver, 172.34.51.225, 44077, None)
"
1740516070271,"INFO	2025-02-25T20:41:10,271	5499	org.apache.spark.storage.BlockManager	[Thread-7]	60	Initialized BlockManager: BlockManagerId(driver, 172.34.51.225, 44077, None)
"
1740516070279,"INFO	2025-02-25T20:41:10,278	5506	org.apache.spark.deploy.glue.client.GlueTaskGroupRMClient	[executorAllocator]	60	Sending request to JES
"
1740516070478,"INFO	2025-02-25T20:41:10,478	5706	org.apache.spark.metrics.sink.GlueCloudwatchSink	[Thread-7]	16	CloudwatchSink: jobName: Hackathon-Test-Glue-2 jobRunId: jr_4b568db588200719ca859067253367b1f480062e908191c64a05ab0ab92739ae
"
1740516070643,"INFO	2025-02-25T20:41:10,643	5871	org.apache.spark.deploy.history.SingleEventLogFileWriter	[Thread-7]	60	Logging events to file:/var/log/spark/apps/spark-application-1740516070192.inprogress
"
1740516070745,"INFO	2025-02-25T20:41:10,745	5973	org.apache.spark.util.Utils	[Thread-7]	60	Using initial executors = 1, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances
"
1740516070746,"INFO	2025-02-25T20:41:10,746	5974	org.apache.spark.ExecutorAllocationManager	[Thread-7]	60	Dynamic allocation is enabled without a shuffle service.
"
1740516070759,"INFO	2025-02-25T20:41:10,759	5987	org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend	[Thread-7]	60	Set total expected executors: rpId: 0, numExecs: 1
"
1740516070759,"INFO	2025-02-25T20:41:10,759	5987	org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend	[Thread-7]	60	Requested total executors are 1
"
1740516070796,"INFO	2025-02-25T20:41:10,795	6023	com.amazonaws.services.glueexceptionanalysis.EventLogFileWriter	[Thread-7]	51	Started file writer for com.amazonaws.services.glueexceptionanalysis.EventLogFileWriter
"
1740516070803,"INFO	2025-02-25T20:41:10,802	6030	org.apache.spark.SparkContext	[Thread-7]	60	Registered listener com.amazonaws.services.glueexceptionanalysis.GlueExceptionAnalysisListener
"
1740516070818,"INFO	2025-02-25T20:41:10,817	6045	org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend	[Thread-7]	60	getDriverLogUrls Map()
"
1740516070822,"INFO	2025-02-25T20:41:10,822	6050	org.apache.spark.executor.ExecutorLogUrlHandler	[Thread-7]	60	Fail to renew executor log urls: some of required attributes are missing in app's event log.. Required: Set(CONTAINER_ID) / available: Set(). Falling back to show app's original log urls.
"
1740516070825,"INFO	2025-02-25T20:41:10,825	6053	org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend	[Thread-7]	60	SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
"
1740516070962,"INFO	2025-02-25T20:41:10,961	6189	org.apache.spark.deploy.glue.client.GlueTaskGroupRMClient	[executorAllocator]	60	createChildTask API response code 200
"
1740516070963,"INFO	2025-02-25T20:41:10,962	6190	org.apache.spark.scheduler.cluster.glue.allocator.ExecutorTaskAllocator	[executorAllocator]	60	executor task g-f9b64d3e7d5138203fd7b8c49d7e1785ed981a0f created for executor 1 in resource profile 0
"
1740516071016,"INFO	2025-02-25T20:41:11,016	6244	com.amazonaws.services.glue.GlueContext	[Thread-7]	119	GlueMetrics configured and enabled
"
1740516071018,"INFO	2025-02-25T20:41:11,018	6246	org.apache.spark.metrics.source.ObservabilityTaskInfoRecorderListener	[Thread-7]	28	ThroughputMetricsSource is initiated
"
1740516071026,"INFO	2025-02-25T20:41:11,026	6254	org.apache.spark.metrics.source.ObservabilityTaskInfoRecorderListener	[Thread-7]	53	ResourceUtilizationMetricsSource is initiated
"
1740516071027,"INFO	2025-02-25T20:41:11,027	6255	org.apache.spark.metrics.source.StageSkewness	[Thread-7]	29	[Observability] Skewness metric using Skewness Factor = 5
"
1740516071028,"INFO	2025-02-25T20:41:11,028	6256	org.apache.spark.metrics.source.ObservabilityTaskInfoRecorderListener	[Thread-7]	68	PerformanceMetricsSource is initiated
"
1740516071028,"INFO	2025-02-25T20:41:11,028	6256	com.amazonaws.services.glue.GlueContext	[Thread-7]	127	ObservabilityMetrics configured and enabled
"
1740516071038,"INFO	2025-02-25T20:41:11,038	6266	com.amazonaws.services.glue.utils.EndpointConfig$	[Thread-7]	36	 STAGE is prod
"
1740516071039,"INFO	2025-02-25T20:41:11,039	6267	com.amazonaws.services.glue.utils.EndpointConfig$	[Thread-7]	90	Endpoints: {credentials_provider=com.amazonaws.auth.DefaultAWSCredentialsProviderChain, glue.endpoint=https://glue.us-east-1.amazonaws.com, lakeformation.endpoint=https://lakeformation.us-east-1.amazonaws.com, jes.endpoint=https://glue-jes-prod.us-east-1.amazonaws.com, region=us-east-1}
"
1740516071085,"INFO	2025-02-25T20:41:11,084	6312	org.apache.spark.sql.internal.SharedState	[Thread-7]	60	spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
"
1740516071087,"INFO	2025-02-25T20:41:11,087	6315	org.apache.spark.sql.internal.SharedState	[Thread-7]	60	Warehouse path is 'file:/tmp/spark-warehouse'.
"
1740516071855,"INFO	2025-02-25T20:41:11,855	7083	org.opensearch.hadoop.util.Version	[Thread-7]	143	OpenSearch Hadoop v1.2.0 [2a4148055f]
"
1740516072030,"INFO	2025-02-25T20:41:12,030	7258	com.amazon.ws.emr.hadoop.fs.guice.DefaultAWSCredentialsProviderFactory	[Thread-7]	72	Unable to create provider using constructor: DefaultAWSCredentialsProviderChain(java.net.URI, org.apache.hadoop.conf.Configuration)
"
1740516072031,"INFO	2025-02-25T20:41:12,030	7258	com.amazon.ws.emr.hadoop.fs.util.ClientConfigurationFactory	[Thread-7]	85	Set initial getObject socket timeout to 2000 ms.
"
1740516072435,"INFO	2025-02-25T20:41:12,435	7663	org.apache.spark.sql.execution.datasources.InMemoryFileIndex	[Thread-7]	60	It took 31 ms to list leaf files for 1 paths.
"
1740516072484,"INFO	2025-02-25T20:41:12,484	7712	org.apache.spark.sql.execution.datasources.InMemoryFileIndex	[Thread-7]	60	It took 2 ms to list leaf files for 1 paths.
"
1740516074444,"INFO	2025-02-25T20:41:14,444	9672	org.apache.spark.sql.execution.datasources.FileSourceStrategy	[Thread-7]	60	Pushed Filters: 
"
1740516074445,"INFO	2025-02-25T20:41:14,445	9673	org.apache.spark.sql.execution.datasources.FileSourceStrategy	[Thread-7]	60	Post-Scan Filters: (length(trim(value#0, None)) > 0)
"
1740516074730,"INFO	2025-02-25T20:41:14,730	9958	org.apache.spark.storage.memory.MemoryStore	[Thread-7]	60	Block broadcast_0 stored as values in memory (estimated size 292.4 KiB, free 11.8 GiB)
"
1740516074794,"INFO	2025-02-25T20:41:14,794	10022	org.apache.spark.storage.memory.MemoryStore	[Thread-7]	60	Block broadcast_0_piece0 stored as bytes in memory (estimated size 55.5 KiB, actual size: 55.5 KiB, free 11.8 GiB)
"
1740516074797,"INFO	2025-02-25T20:41:14,797	10025	org.apache.spark.storage.BlockManagerInfo	[dispatcher-BlockManagerMaster]	60	Added broadcast_0_piece0 in memory on 172.34.51.225:44077 (size: 55.5 KiB, free: 11.8 GiB)
"
1740516074802,"INFO	2025-02-25T20:41:14,802	10030	org.apache.spark.SparkContext	[Thread-7]	60	Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
"
1740516074824,"INFO	2025-02-25T20:41:14,824	10052	com.hadoop.compression.lzo.GPLNativeCodeLoader	[Thread-7]	34	Loaded native gpl library
"
1740516074826,"INFO	2025-02-25T20:41:14,826	10054	com.hadoop.compression.lzo.LzoCodec	[Thread-7]	76	Successfully loaded & initialized native-lzo library [hadoop-lzo rev 049362b7cf53ff5f739d6b1532457f2c6cd495e8]
"
1740516074845,"INFO	2025-02-25T20:41:14,845	10073	org.apache.spark.sql.execution.FileSourceScanExec	[Thread-7]	60	Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
"
1740516074850,"INFO	2025-02-25T20:41:14,850	10078	org.apache.spark.sql.execution.FileSourceScanExec	[Thread-7]	60	relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
"
1740516075047,"INFO	2025-02-25T20:41:15,047	10275	org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator	[Thread-7]	60	Code generated in 153.35567 ms
"
1740516075113,"INFO	2025-02-25T20:41:15,113	10341	org.apache.spark.SparkContext	[Thread-7]	60	Starting job: csv at NativeMethodAccessorImpl.java:0
"
1740516075130,"INFO	2025-02-25T20:41:15,129	10357	org.apache.spark.scheduler.DAGScheduler	[dag-scheduler-event-loop]	60	Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
"
1740516075130,"INFO	2025-02-25T20:41:15,130	10358	org.apache.spark.scheduler.DAGScheduler	[dag-scheduler-event-loop]	60	Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
"
1740516075130,"INFO	2025-02-25T20:41:15,130	10358	org.apache.spark.scheduler.DAGScheduler	[dag-scheduler-event-loop]	60	Parents of final stage: List()
"
1740516075132,"INFO	2025-02-25T20:41:15,131	10359	org.apache.spark.scheduler.DAGScheduler	[dag-scheduler-event-loop]	60	Missing parents: List()
"
1740516075136,"INFO	2025-02-25T20:41:15,136	10364	org.apache.spark.scheduler.DAGScheduler	[dag-scheduler-event-loop]	60	Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
"
1740516075234,"INFO	2025-02-25T20:41:15,234	10462	org.apache.spark.storage.memory.MemoryStore	[dag-scheduler-event-loop]	60	Block broadcast_1 stored as values in memory (estimated size 16.9 KiB, free 11.8 GiB)
"
1740516075236,"INFO	2025-02-25T20:41:15,236	10464	org.apache.spark.storage.memory.MemoryStore	[dag-scheduler-event-loop]	60	Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.3 KiB, actual size: 8.3 KiB, free 11.8 GiB)
"
1740516075237,"INFO	2025-02-25T20:41:15,237	10465	org.apache.spark.storage.BlockManagerInfo	[dispatcher-BlockManagerMaster]	60	Added broadcast_1_piece0 in memory on 172.34.51.225:44077 (size: 8.3 KiB, free: 11.8 GiB)
"
1740516075238,"INFO	2025-02-25T20:41:15,238	10466	org.apache.spark.SparkContext	[dag-scheduler-event-loop]	60	Created broadcast 1 from broadcast at DAGScheduler.scala:1664
"
1740516075254,"INFO	2025-02-25T20:41:15,254	10482	org.apache.spark.scheduler.DAGScheduler	[dag-scheduler-event-loop]	60	Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
"
1740516075255,"INFO	2025-02-25T20:41:15,255	10483	org.apache.spark.scheduler.TaskSchedulerImpl	[dag-scheduler-event-loop]	60	Adding task set 0.0 with 1 tasks resource profile 0
"
1740516090266,"WARN	2025-02-25T20:41:30,266	25494	org.apache.spark.scheduler.TaskSchedulerImpl	[task-starvation-timer]	72	Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
"
1740516091190,"INFO	2025-02-25T20:41:31,189	26417	org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend$JESAsSchedulerBackendEndpoint	[dispatcher-CoarseGrainedScheduler]	60	Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.38.3.104:53038) with ID 1,  ResourceProfileId 0
"
1740516091192,"INFO	2025-02-25T20:41:31,191	26419	org.apache.spark.executor.ExecutorLogUrlHandler	[dispatcher-CoarseGrainedScheduler]	60	Fail to renew executor log urls: some of required attributes are missing in app's event log.. Required: Set(CONTAINER_ID) / available: Set(). Falling back to show app's original log urls.
"
1740516091193,"INFO	2025-02-25T20:41:31,192	26420	org.apache.spark.scheduler.cluster.glue.ExecutorEventListener	[spark-listener-group-shared]	60	Got executor added event for 1 @ 1740516091192
"
1740516091193,"INFO	2025-02-25T20:41:31,193	26421	org.apache.spark.scheduler.cluster.glue.allocator.ExecutorTaskAllocator	[spark-listener-group-shared]	60	connected executor 1
"
1740516091196,"INFO	2025-02-25T20:41:31,196	26424	org.apache.spark.scheduler.dynalloc.ExecutorMonitor	[spark-listener-group-executorManagement]	60	New executor 1 has registered (new total is 1)
"
1740516091261,"INFO	2025-02-25T20:41:31,261	26489	org.apache.spark.storage.BlockManagerMasterEndpoint	[dispatcher-BlockManagerMaster]	60	Registering block manager 172.38.3.104:38343 with 11.8 GiB RAM, BlockManagerId(1, 172.38.3.104, 38343, None)
"
1740516092513,"INFO	2025-02-25T20:41:32,512	27740	org.apache.spark.scheduler.TaskSetManager	[dispatcher-CoarseGrainedScheduler]	60	Starting task 0.0 in stage 0.0 (TID 0) (172.38.3.104, executor 1, partition 0, ANY, 10424 bytes) 
"
1740516093148,"INFO	2025-02-25T20:41:33,148	28376	org.apache.spark.storage.BlockManagerInfo	[dispatcher-BlockManagerMaster]	60	Added broadcast_1_piece0 in memory on 172.38.3.104:38343 (size: 8.3 KiB, free: 11.8 GiB)
"
1740516095377,"INFO	2025-02-25T20:41:35,377	30605	org.apache.spark.storage.BlockManagerInfo	[dispatcher-BlockManagerMaster]	60	Added broadcast_0_piece0 in memory on 172.38.3.104:38343 (size: 55.5 KiB, free: 11.8 GiB)
"
1740516097147,"INFO	2025-02-25T20:41:37,147	32375	org.apache.spark.scheduler.TaskSetManager	[task-result-getter-0]	60	Finished task 0.0 in stage 0.0 (TID 0) in 4646 ms on 172.38.3.104 (executor 1) (1/1)
"
1740516097149,"INFO	2025-02-25T20:41:37,148	32376	org.apache.spark.scheduler.TaskSchedulerImpl	[task-result-getter-0]	60	Removed TaskSet 0.0, whose tasks have all completed, from pool 
"
1740516097152,"INFO	2025-02-25T20:41:37,152	32380	org.apache.spark.scheduler.DAGScheduler	[dag-scheduler-event-loop]	60	ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 22.002 s
"
1740516097157,"INFO	2025-02-25T20:41:37,156	32384	org.apache.spark.scheduler.DAGScheduler	[dag-scheduler-event-loop]	60	Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
"
1740516097157,"INFO	2025-02-25T20:41:37,157	32385	org.apache.spark.scheduler.TaskSchedulerImpl	[dag-scheduler-event-loop]	60	Killing all running tasks in stage 0: Stage finished
"
1740516097160,"INFO	2025-02-25T20:41:37,160	32388	org.apache.spark.scheduler.DAGScheduler	[Thread-7]	60	Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 22.046237 s
"
1740516097184,"INFO	2025-02-25T20:41:37,184	32412	org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator	[Thread-7]	60	Code generated in 11.922955 ms
"
1740516097195,"INFO	2025-02-25T20:41:37,194	32422	org.apache.spark.sql.execution.SQLExecution	[Thread-7]	60	Generating and posting SparkListenerSQLExecutionObfuscatedInfo...
"
1740516097197,"INFO	2025-02-25T20:41:37,197	32425	org.apache.spark.sql.execution.SQLExecution	[Thread-7]	60	Posted SparkListenerSQLExecutionObfuscatedInfo in 3 ms
"
1740516097251,"INFO	2025-02-25T20:41:37,251	32479	org.apache.spark.sql.execution.datasources.FileSourceStrategy	[Thread-7]	60	Pushed Filters: 
"
1740516097251,"INFO	2025-02-25T20:41:37,251	32479	org.apache.spark.sql.execution.datasources.FileSourceStrategy	[Thread-7]	60	Post-Scan Filters: 
"
1740516097259,"INFO	2025-02-25T20:41:37,259	32487	org.apache.spark.storage.memory.MemoryStore	[Thread-7]	60	Block broadcast_2 stored as values in memory (estimated size 292.4 KiB, free 11.8 GiB)
"
1740516097272,"INFO	2025-02-25T20:41:37,271	32499	org.apache.spark.storage.memory.MemoryStore	[Thread-7]	60	Block broadcast_2_piece0 stored as bytes in memory (estimated size 55.5 KiB, actual size: 55.5 KiB, free 11.8 GiB)
"
1740516097272,"INFO	2025-02-25T20:41:37,272	32500	org.apache.spark.storage.BlockManagerInfo	[dispatcher-BlockManagerMaster]	60	Added broadcast_2_piece0 in memory on 172.34.51.225:44077 (size: 55.5 KiB, free: 11.8 GiB)
"
1740516097273,"INFO	2025-02-25T20:41:37,273	32501	org.apache.spark.SparkContext	[Thread-7]	60	Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
"
1740516097274,"INFO	2025-02-25T20:41:37,274	32502	org.apache.spark.sql.execution.FileSourceScanExec	[Thread-7]	60	Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
"
1740516097274,"INFO	2025-02-25T20:41:37,274	32502	org.apache.spark.sql.execution.FileSourceScanExec	[Thread-7]	60	relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
"
1740516097357,"INFO	2025-02-25T20:41:37,357	32585	org.apache.spark.sql.execution.datasources.FileSourceStrategy	[Thread-7]	60	Pushed Filters: 
"
1740516097357,"INFO	2025-02-25T20:41:37,357	32585	org.apache.spark.sql.execution.datasources.FileSourceStrategy	[Thread-7]	60	Post-Scan Filters: 
"
1740516097369,"INFO	2025-02-25T20:41:37,369	32597	org.apache.spark.metrics.source.ThroughputMetricsSource	[spark-listener-group-shared]	30	Metric: s3://gifs-infrastructure-ai-hackathon/mock-data/kaggle-dataset/NVDA2.csv.filesRead is already registered by a different accumulator. Retrying with suffix #1.
"
1740516097369,"INFO	2025-02-25T20:41:37,369	32597	org.apache.spark.metrics.source.ThroughputMetricsSource	[spark-listener-group-shared]	30	Metric: s3://gifs-infrastructure-ai-hackathon/mock-data/kaggle-dataset/NVDA2.csv.bytesRead is already registered by a different accumulator. Retrying with suffix #1.
"
1740516097387,"INFO	2025-02-25T20:41:37,370	32598	org.apache.spark.metrics.source.ThroughputMetricsSource	[spark-listener-group-shared]	30	Metric: s3://gifs-infrastructure-ai-hackathon/mock-data/kaggle-dataset/NVDA2.csv.recordsRead is already registered by a different accumulator. Retrying with suffix #1.
"
1740516097406,"INFO	2025-02-25T20:41:37,405	32633	org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator	[Thread-7]	60	Code generated in 14.797934 ms
"
1740516097411,"INFO	2025-02-25T20:41:37,410	32638	org.apache.spark.storage.memory.MemoryStore	[Thread-7]	60	Block broadcast_3 stored as values in memory (estimated size 292.3 KiB, free 11.8 GiB)
"
1740516097423,"INFO	2025-02-25T20:41:37,422	32650	org.apache.spark.storage.memory.MemoryStore	[Thread-7]	60	Block broadcast_3_piece0 stored as bytes in memory (estimated size 55.4 KiB, actual size: 55.4 KiB, free 11.8 GiB)
"
1740516097423,"INFO	2025-02-25T20:41:37,423	32651	org.apache.spark.storage.BlockManagerInfo	[dispatcher-BlockManagerMaster]	60	Added broadcast_3_piece0 in memory on 172.34.51.225:44077 (size: 55.4 KiB, free: 11.8 GiB)
"
1740516097424,"INFO	2025-02-25T20:41:37,424	32652	org.apache.spark.SparkContext	[Thread-7]	60	Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0
"
1740516097430,"INFO	2025-02-25T20:41:37,430	32658	org.apache.spark.sql.execution.FileSourceScanExec	[Thread-7]	60	Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false
"
1740516097430,"INFO	2025-02-25T20:41:37,430	32658	org.apache.spark.sql.execution.FileSourceScanExec	[Thread-7]	60	relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))
"
1740516097441,"INFO	2025-02-25T20:41:37,441	32669	org.apache.spark.SparkContext	[Thread-7]	60	Starting job: showString at NativeMethodAccessorImpl.java:0
"
1740516097442,"INFO	2025-02-25T20:41:37,441	32669	org.apache.spark.scheduler.DAGScheduler	[dag-scheduler-event-loop]	60	Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
"
1740516097442,"INFO	2025-02-25T20:41:37,442	32670	org.apache.spark.scheduler.DAGScheduler	[dag-scheduler-event-loop]	60	Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
"
1740516097442,"INFO	2025-02-25T20:41:37,442	32670	org.apache.spark.scheduler.DAGScheduler	[dag-scheduler-event-loop]	60	Parents of final stage: List()
"
1740516097442,"INFO	2025-02-25T20:41:37,442	32670	org.apache.spark.scheduler.DAGScheduler	[dag-scheduler-event-loop]	60	Missing parents: List()
"
1740516097443,"INFO	2025-02-25T20:41:37,442	32670	org.apache.spark.scheduler.DAGScheduler	[dag-scheduler-event-loop]	60	Submitting ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
"
1740516097447,"INFO	2025-02-25T20:41:37,447	32675	org.apache.spark.storage.memory.MemoryStore	[dag-scheduler-event-loop]	60	Block broadcast_4 stored as values in memory (estimated size 20.8 KiB, free 11.8 GiB)
"
1740516097449,"INFO	2025-02-25T20:41:37,449	32677	org.apache.spark.storage.memory.MemoryStore	[dag-scheduler-event-loop]	60	Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.5 KiB, actual size: 9.5 KiB, free 11.8 GiB)
"
1740516097450,"INFO	2025-02-25T20:41:37,450	32678	org.apache.spark.storage.BlockManagerInfo	[dispatcher-BlockManagerMaster]	60	Added broadcast_4_piece0 in memory on 172.34.51.225:44077 (size: 9.5 KiB, free: 11.8 GiB)
"
1740516097451,"INFO	2025-02-25T20:41:37,450	32678	org.apache.spark.SparkContext	[dag-scheduler-event-loop]	60	Created broadcast 4 from broadcast at DAGScheduler.scala:1664
"
1740516097451,"INFO	2025-02-25T20:41:37,451	32679	org.apache.spark.scheduler.DAGScheduler	[dag-scheduler-event-loop]	60	Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
"
1740516097452,"INFO	2025-02-25T20:41:37,451	32679	org.apache.spark.scheduler.TaskSchedulerImpl	[dag-scheduler-event-loop]	60	Adding task set 1.0 with 1 tasks resource profile 0
"
1740516097469,"INFO	2025-02-25T20:41:37,453	32681	org.apache.spark.scheduler.TaskSetManager	[dispatcher-CoarseGrainedScheduler]	60	Starting task 0.0 in stage 1.0 (TID 1) (172.38.3.104, executor 1, partition 0, ANY, 10424 bytes) 
"
1740516097473,"INFO	2025-02-25T20:41:37,472	32700	org.apache.spark.storage.BlockManagerInfo	[dispatcher-BlockManagerMaster]	60	Added broadcast_4_piece0 in memory on 172.38.3.104:38343 (size: 9.5 KiB, free: 11.8 GiB)
"
1740516097574,"INFO	2025-02-25T20:41:37,574	32802	org.apache.spark.storage.BlockManagerInfo	[dispatcher-BlockManagerMaster]	60	Added broadcast_3_piece0 in memory on 172.38.3.104:38343 (size: 55.4 KiB, free: 11.8 GiB)
"
1740516097622,"INFO	2025-02-25T20:41:37,622	32850	com.amazonaws.services.glue.LogPusher	[pool-6-thread-1]	60	uploading file:///var/log/spark/apps to s3://aws-glue-assets-842675997893-us-east-1/sparkHistoryLogs/
"
1740516097754,"INFO	2025-02-25T20:41:37,754	32982	org.apache.spark.scheduler.TaskSetManager	[task-result-getter-1]	60	Finished task 0.0 in stage 1.0 (TID 1) in 302 ms on 172.38.3.104 (executor 1) (1/1)
"
1740516097754,"INFO	2025-02-25T20:41:37,754	32982	org.apache.spark.scheduler.TaskSchedulerImpl	[task-result-getter-1]	60	Removed TaskSet 1.0, whose tasks have all completed, from pool 
"
1740516097756,"INFO	2025-02-25T20:41:37,756	32984	org.apache.spark.scheduler.DAGScheduler	[dag-scheduler-event-loop]	60	ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.311 s
"
1740516097756,"INFO	2025-02-25T20:41:37,756	32984	org.apache.spark.scheduler.DAGScheduler	[dag-scheduler-event-loop]	60	Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
"
1740516097756,"INFO	2025-02-25T20:41:37,756	32984	org.apache.spark.scheduler.TaskSchedulerImpl	[dag-scheduler-event-loop]	60	Killing all running tasks in stage 1: Stage finished
"
1740516097757,"INFO	2025-02-25T20:41:37,757	32985	org.apache.spark.scheduler.DAGScheduler	[Thread-7]	60	Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.316151 s
"
1740516097770,"ERROR	2025-02-25T20:41:37,764	32992	com.amazonaws.services.glue.LogPusher	[pool-6-thread-1]	97	com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: WN3J4WHYBSE5RYMG; S3 Extended Request ID: EFIa5YGM/0WF1Z9+3s29lHUMlb1DAp7Qu/6tpX2nUDk9zGT3QUrQD6tjDAk0JRIhSejhQWpXKsM=; Proxy: null), S3 Extended Request ID: EFIa5YGM/0WF1Z9+3s29lHUMlb1DAp7Qu/6tpX2nUDk9zGT3QUrQD6tjDAk0JRIhSejhQWpXKsM=
java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: WN3J4WHYBSE5RYMG; S3 Extended Request ID: EFIa5YGM/0WF1Z9+3s29lHUMlb1DAp7Qu/6tpX2nUDk9zGT3QUrQD6tjDAk0JRIhSejhQWpXKsM=; Proxy: null), S3 Extended Request ID: EFIa5YGM/0WF1Z9+3s29lHUMlb1DAp7Qu/6tpX2nUDk9zGT3QUrQD6tjDAk0JRIhSejhQWpXKsM=
	at com.amazon.ws.emr.hadoop.fs.s3n.Jets"
1740516097770,"3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:429) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolderUsingFolderObject(Jets3tNativeFileSystemStore.java:255) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolder(Jets3tNativeFileSystemStore.java:218) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:554) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at org.apache.hadoop.fs.FileUtil.checkDest(FileUtil.java:611) ~[hadoop-client-api-3.4.0-amzn-1.jar:?]
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:468) ~[hadoop-client-api-3.4.0-amzn-1.jar:?]
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:432) ~[hadoop-client-api-3.4.0-amzn-1.jar:?]
	at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:2647) ~[hadoop-client-api-3.4.0-amzn-1.jar:?]
	at org.apache.hadoop.fs.FileSystem.copyFromLocalFile"
1740516097770,"(FileSystem.java:2613) ~[hadoop-client-api-3.4.0-amzn-1.jar:?]
	at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:2575) ~[hadoop-client-api-3.4.0-amzn-1.jar:?]
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.copyFromLocalFile(EmrFileSystem.java:518) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazonaws.services.glue.LogPusher.upload(LogPusher.scala:72) [UcyzHl-aws-glue-di-package-5.0.349.jar:?]
	at com.amazonaws.services.glue.LogPusher.run(LogPusher.scala:93) [UcyzHl-aws-glue-di-package-5.0.349.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305) [?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840"
1740516097770,") [?:?]
Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: WN3J4WHYBSE5RYMG; S3 Extended Request ID: EFIa5YGM/0WF1Z9+3s29lHUMlb1DAp7Qu/6tpX2nUDk9zGT3QUrQD6tjDAk0JRIhSejhQWpXKsM=; Proxy: null)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157) ~[em"
1740516097770,"rfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561) ~[emrfs-hadoop-assembly-2.66.0.jar"
1740516097770,":?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5558) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5505) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:1002) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:26) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:12) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor$CallPerformer.call(GlobalS3Executor.java:114) ~[emrfs-hadoop-assembly-2.66"
1740516097770,".0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:141) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:196) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:420) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	... 18 more
"
1740516097784,"INFO	2025-02-25T20:41:37,784	33012	org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator	[Thread-7]	60	Code generated in 15.237928 ms
"
1740516097787,"INFO	2025-02-25T20:41:37,787	33015	org.apache.spark.sql.execution.SQLExecution	[Thread-7]	60	Generating and posting SparkListenerSQLExecutionObfuscatedInfo...
"
1740516097787,"INFO	2025-02-25T20:41:37,787	33015	org.apache.spark.sql.execution.SQLExecution	[Thread-7]	60	Posted SparkListenerSQLExecutionObfuscatedInfo in 1 ms
"
1740516097832,"INFO	2025-02-25T20:41:37,831	33059	com.amazonaws.services.glue.ProcessLauncher	[main]	60	postprocessing
"
1740516097832,"INFO	2025-02-25T20:41:37,832	33060	com.amazonaws.services.glue.ProcessLauncher	[main]	633	Enhance failure reason and emit cloudwatch error metrics.
"
1740516097848,"INFO	2025-02-25T20:41:37,848	33076	com.amazonaws.services.glue.CloudWatchMetricsEmitter	[main]	34	Emit job error metrics
"
1740516097975,"INFO	2025-02-25T20:41:37,974	33202	com.amazonaws.services.glue.LogPusher	[main]	60	stopping
"
1740516097977,"INFO	2025-02-25T20:41:37,977	33205	org.apache.spark.SparkContext	[shutdown-hook-0]	60	Invoking stop() from shutdown hook
"
1740516097977,"INFO	2025-02-25T20:41:37,977	33205	org.apache.spark.SparkContext	[shutdown-hook-0]	60	SparkContext is stopping with exitCode 0.
"
1740516097981,"INFO	2025-02-25T20:41:37,981	33209	org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend	[shutdown-hook-0]	60	Stopping JES Scheduler Backend.
"
1740516097982,"INFO	2025-02-25T20:41:37,982	33210	org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend	[shutdown-hook-0]	60	Shutting down all executors
"
1740516097982,"INFO	2025-02-25T20:41:37,982	33210	org.apache.spark.scheduler.cluster.glue.JESSchedulerBackend$JESAsSchedulerBackendEndpoint	[dispatcher-CoarseGrainedScheduler]	60	Asking each executor to shut down
"
1740516097983,"INFO	2025-02-25T20:41:37,983	33211	com.amazonaws.services.glueexceptionanalysis.EventLogFileWriter	[spark-listener-group-shared]	70	Logs, events processed and insights are written to file /tmp/glue-exception-analysis-logs/spark-application-1740516070192
"
1740516097992,"INFO	2025-02-25T20:41:37,992	33220	org.apache.spark.MapOutputTrackerMasterEndpoint	[dispatcher-event-loop-6]	60	MapOutputTrackerMasterEndpoint stopped!
"
1740516098004,"INFO	2025-02-25T20:41:38,003	33231	org.apache.spark.storage.memory.MemoryStore	[shutdown-hook-0]	60	MemoryStore cleared
"
1740516098004,"INFO	2025-02-25T20:41:38,004	33232	org.apache.spark.storage.BlockManager	[shutdown-hook-0]	60	BlockManager stopped
"
1740516098010,"INFO	2025-02-25T20:41:38,010	33238	org.apache.spark.storage.BlockManagerMaster	[shutdown-hook-0]	60	BlockManagerMaster stopped
"
1740516098367,"INFO	2025-02-25T20:41:38,366	33594	org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint	[dispatcher-event-loop-5]	60	OutputCommitCoordinator stopped!
"
1740516098405,"INFO	2025-02-25T20:41:38,404	33632	org.apache.spark.SparkContext	[shutdown-hook-0]	60	Successfully stopped SparkContext
"
1740516098405,"INFO	2025-02-25T20:41:38,405	33633	com.amazonaws.services.glue.LogPusher	[shutdown-hook-0]	60	uploading file:///var/log/spark/apps to s3://aws-glue-assets-842675997893-us-east-1/sparkHistoryLogs/
"
1740516098437,"ERROR	2025-02-25T20:41:38,436	33664	com.amazonaws.services.glue.LogPusher	[shutdown-hook-0]	97	com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: PBQ2YSDBBZ3PRSRS; S3 Extended Request ID: gV3euzJgkMLa/b1cUi0tNlcRsNl5wPWEU4hc1sSMaUjst2mZKGuxm5w56iUqBpl641XWDKGZdtU=; Proxy: null), S3 Extended Request ID: gV3euzJgkMLa/b1cUi0tNlcRsNl5wPWEU4hc1sSMaUjst2mZKGuxm5w56iUqBpl641XWDKGZdtU=
java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: PBQ2YSDBBZ3PRSRS; S3 Extended Request ID: gV3euzJgkMLa/b1cUi0tNlcRsNl5wPWEU4hc1sSMaUjst2mZKGuxm5w56iUqBpl641XWDKGZdtU=; Proxy: null), S3 Extended Request ID: gV3euzJgkMLa/b1cUi0tNlcRsNl5wPWEU4hc1sSMaUjst2mZKGuxm5w56iUqBpl641XWDKGZdtU=
	at com.amazon.ws.emr.hadoop.fs.s3n.Jets"
1740516098437,"3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:429) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolderUsingFolderObject(Jets3tNativeFileSystemStore.java:255) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolder(Jets3tNativeFileSystemStore.java:218) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:554) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at org.apache.hadoop.fs.FileUtil.checkDest(FileUtil.java:611) ~[hadoop-client-api-3.4.0-amzn-1.jar:?]
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:468) ~[hadoop-client-api-3.4.0-amzn-1.jar:?]
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:432) ~[hadoop-client-api-3.4.0-amzn-1.jar:?]
	at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:2647) ~[hadoop-client-api-3.4.0-amzn-1.jar:?]
	at org.apache.hadoop.fs.FileSystem.copyFromLocalFile"
1740516098437,"(FileSystem.java:2613) ~[hadoop-client-api-3.4.0-amzn-1.jar:?]
	at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:2575) ~[hadoop-client-api-3.4.0-amzn-1.jar:?]
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.copyFromLocalFile(EmrFileSystem.java:518) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazonaws.services.glue.LogPusher.upload(LogPusher.scala:72) ~[UcyzHl-aws-glue-di-package-5.0.349.jar:?]
	at org.apache.spark.util.ShutdownHookManagerWrapper$.$anonfun$addLogPusherHook$2(ShutdownHookManagerWrapper.scala:9) ~[UcyzHl-aws-glue-di-package-5.0.349.jar:3.5.2-amzn-1]
	at org.apache.spark.util.ShutdownHookManagerWrapper$.$anonfun$addLogPusherHook$2$adapted(ShutdownHookManagerWrapper.scala:9) ~[UcyzHl-aws-glue-di-package-5.0.349.jar:3.5.2-amzn-1]
	at scala.Option.foreach(Option.scala:407) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.util.ShutdownHookManagerWrapper$.$anonfun$addLogPusherHook$1(ShutdownHookManagerWrapper.scala:9) ~[UcyzHl-aws-glue-di-package-5.0.349.jar:3.5.2-amzn-1]
	at o"
1740516098437,"rg.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214) ~[spark-core_2.12-3.5.2-amzn-1.jar:3.5.2-amzn-1]
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188) ~[spark-core_2.12-3.5.2-amzn-1.jar:3.5.2-amzn-1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1971) ~[spark-core_2.12-3.5.2-amzn-1.jar:3.5.2-amzn-1]
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188) ~[spark-core_2.12-3.5.2-amzn-1.jar:3.5.2-amzn-1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) [scala-library-2.12.18.jar:?]
	at scala.util.Try$.apply(Try.scala:213) [scala-library-2.12.18.jar:?]
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188) [spark-core_2.12-3.5.2-amzn-1.jar:3.5.2-amzn-1]
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run"
1740516098437,"(ShutdownHookManager.scala:178) [spark-core_2.12-3.5.2-amzn-1.jar:3.5.2-amzn-1]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: PBQ2YSDBBZ3PRSRS; S3 Extended Request ID: gV3euzJgkMLa/b1cUi0tNlcRsNl5wPWEU4hc1sSMaUjst2mZKGuxm5w56iUqBpl641XWDKGZdtU=; Proxy: null)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amaz"
1740516098437,"onaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded"
1740516098452,".com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5558) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5505) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:1002) ~"
1740516098453,"[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:26) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:12) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor$CallPerformer.call(GlobalS3Executor.java:114) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:141) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:196) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	at com.amazo"
1740516098453,"n.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:420) ~[emrfs-hadoop-assembly-2.66.0.jar:?]
	... 29 more
"
1740516098453,"INFO	2025-02-25T20:41:38,437	33665	org.apache.spark.util.ShutdownHookManager	[shutdown-hook-0]	60	Shutdown hook called
"
1740516098453,"INFO	2025-02-25T20:41:38,438	33666	org.apache.spark.util.ShutdownHookManager	[shutdown-hook-0]	60	Deleting directory /tmp/spark-e84c38a7-a043-47af-bf1a-bd1abb97139d
"
1740516098453,"INFO	2025-02-25T20:41:38,442	33670	org.apache.spark.util.ShutdownHookManager	[shutdown-hook-0]	60	Deleting directory /tmp/spark-497ee5a4-ac26-4330-996c-d9b518a53f73
"
1740516098453,"INFO	2025-02-25T20:41:38,447	33675	org.apache.spark.util.ShutdownHookManager	[shutdown-hook-0]	60	Deleting directory /tmp/spark-e84c38a7-a043-47af-bf1a-bd1abb97139d/pyspark-fdb5f44f-3a21-482c-ace6-b3eb3ee28597
"